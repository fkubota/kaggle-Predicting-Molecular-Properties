{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- nb45の編集\n",
    "- type別学習\n",
    "- 以下のカーネルを参考にした\n",
    "    - > keras-neural-net-for-champs.ipynb\n",
    "- NN(keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import everything I need :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import multiprocessing\n",
    "import glob\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# import lightgbm as lgb\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers import BatchNormalization,Add,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 46\n",
    "isSmallSet = True\n",
    "length = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use atomic numbers to recode atomic names\n",
    "ATOMIC_NUMBERS = {\n",
    "    'H': 1,\n",
    "    'C': 6,\n",
    "    'N': 7,\n",
    "    'O': 8,\n",
    "    'F': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/champs-scalar-coupling/scalar_coupling_contributions.csv',\n",
       " '../input/champs-scalar-coupling/magnetic_shielding_tensors.csv',\n",
       " '../input/champs-scalar-coupling/structures.csv',\n",
       " '../input/champs-scalar-coupling/test.csv',\n",
       " '../input/champs-scalar-coupling/dipole_moments.csv',\n",
       " '../input/champs-scalar-coupling/potential_energy.csv',\n",
       " '../input/champs-scalar-coupling/sample_submission.csv',\n",
       " '../input/champs-scalar-coupling/train.csv',\n",
       " '../input/champs-scalar-coupling/nb33_train_dist-interaction.csv',\n",
       " '../input/champs-scalar-coupling/test_ob_charges_V7EstimatioofMullikenChargeswithOpenBabel.csv',\n",
       " '../input/champs-scalar-coupling/nb29_fc_test_feature.csv',\n",
       " '../input/champs-scalar-coupling/train_ob_charges_V7EstimatioofMullikenChargeswithOpenBabel.csv',\n",
       " '../input/champs-scalar-coupling/nb33_test_dist-interaction.csv',\n",
       " '../input/champs-scalar-coupling/mulliken_charges.csv',\n",
       " '../input/champs-scalar-coupling/nb29_fc_train_feature.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../input/champs-scalar-coupling/'\n",
    "glob.glob(file_path + '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "path = file_path + 'train.csv'\n",
    "if isSmallSet:\n",
    "    train = pd.read_csv(path) [:length]\n",
    "else:\n",
    "    train = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "path = file_path + 'test.csv'\n",
    "if isSmallSet:\n",
    "    test = pd.read_csv(path)[:length]\n",
    "else:\n",
    "    test = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure\n",
    "path = file_path + 'structures.csv'\n",
    "structures = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_train\n",
    "path = file_path + 'nb29_fc_train_feature.csv'\n",
    "if isSmallSet:\n",
    "    fc_train = pd.read_csv(path)[:length]\n",
    "else:\n",
    "    fc_train = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_test\n",
    "path = file_path + 'nb29_fc_test_feature.csv'\n",
    "if isSmallSet:\n",
    "    fc_test = pd.read_csv(path)[:length]\n",
    "else:\n",
    "    fc_test = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dist-interact\n",
    "path = file_path + 'nb33_train_dist-interaction.csv'\n",
    "if isSmallSet:\n",
    "    dist_interact_train = pd.read_csv(path)[:length]\n",
    "else:\n",
    "    dist_interact_train = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dist-interact\n",
    "path = file_path + 'nb33_test_dist-interaction.csv'\n",
    "if isSmallSet:\n",
    "    dist_interact_test = pd.read_csv(path)[:length]\n",
    "else:\n",
    "    dist_interact_test = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ob charge train\n",
    "path = file_path + 'train_ob_charges_V7EstimatioofMullikenChargeswithOpenBabel.csv'\n",
    "if isSmallSet:\n",
    "    ob_charge_train = pd.read_csv(path)[:length].drop(['Unnamed: 0', 'error'], axis=1)\n",
    "else:\n",
    "    ob_charge_train = pd.read_csv(path).drop(['Unnamed: 0', 'error'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ob charge test\n",
    "path = file_path + 'test_ob_charges_V7EstimatioofMullikenChargeswithOpenBabel.csv'\n",
    "if isSmallSet:\n",
    "    ob_charge_test = pd.read_csv(path)[:length].drop(['Unnamed: 0', 'error'], axis=1)\n",
    "else:\n",
    "    ob_charge_test = pd.read_csv(path).drop(['Unnamed: 0', 'error'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mulliken charges.csv\n",
    "path = file_path + 'mulliken_charges.csv'\n",
    "charge = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor\n",
    "path = file_path + 'magnetic_shielding_tensors.csv'\n",
    "tensor = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using SmallSet !!\n",
      "-------------------\n",
      "There are 10000 rows in train data.\n",
      "There are 10000 rows in test data.\n",
      "There are 312 distinct molecules in train data.\n",
      "There are 266 distinct molecules in test data.\n",
      "There are 19 unique atoms.\n",
      "There are 8 unique types.\n"
     ]
    }
   ],
   "source": [
    "if isSmallSet:\n",
    "    print('using SmallSet !!')\n",
    "    print('-------------------')\n",
    "\n",
    "print(f'There are {train.shape[0]} rows in train data.')\n",
    "print(f'There are {test.shape[0]} rows in test data.')\n",
    "\n",
    "print(f\"There are {train['molecule_name'].nunique()} distinct molecules in train data.\")\n",
    "print(f\"There are {test['molecule_name'].nunique()} distinct molecules in test data.\")\n",
    "print(f\"There are {train['atom_index_0'].nunique()} unique atoms.\")\n",
    "print(f\"There are {train['type'].nunique()} unique types.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## myFunc\n",
    "**metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_metric(df, preds):\n",
    "    df[\"prediction\"] = preds\n",
    "    maes = []\n",
    "    for t in df.type.unique():\n",
    "        y_true = df[df.type==t].scalar_coupling_constant.values\n",
    "        y_pred = df[df.type==t].prediction.values\n",
    "        mae = np.log(mean_absolute_error(y_true, y_pred))\n",
    "        maes.append(mae)\n",
    "    return np.mean(maes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**momory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Distance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_type_dataframes(base, structures, coupling_type):\n",
    "    base = base[base['type'] == coupling_type].drop('type', axis=1).copy()\n",
    "    base = base.reset_index()\n",
    "    base['id'] = base['id'].astype('int32')\n",
    "    structures = structures[structures['molecule_name'].isin(base['molecule_name'])]\n",
    "    return base, structures\n",
    "\n",
    "# a,b = build_type_dataframes(train, structures, '1JHN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coordinates(base, structures, index):\n",
    "    df = pd.merge(base, structures, how='inner',\n",
    "                  left_on=['molecule_name', f'atom_index_{index}'],\n",
    "                  right_on=['molecule_name', 'atom_index']).drop(['atom_index'], axis=1)\n",
    "    df = df.rename(columns={\n",
    "        'atom': f'atom_{index}',\n",
    "        'x': f'x_{index}',\n",
    "        'y': f'y_{index}',\n",
    "        'z': f'z_{index}'\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_atoms(base, atoms):\n",
    "    df = pd.merge(base, atoms, how='inner',\n",
    "                  on=['molecule_name', 'atom_index_0', 'atom_index_1'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_atoms(base, structures):\n",
    "    df = pd.merge(base, structures, how='left',\n",
    "                  left_on=['molecule_name'],\n",
    "                  right_on=['molecule_name'])\n",
    "    df = df[(df.atom_index_0 != df.atom_index) & (df.atom_index_1 != df.atom_index)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_center(df):\n",
    "    df['x_c'] = ((df['x_1'] + df['x_0']) * np.float32(0.5))\n",
    "    df['y_c'] = ((df['y_1'] + df['y_0']) * np.float32(0.5))\n",
    "    df['z_c'] = ((df['z_1'] + df['z_0']) * np.float32(0.5))\n",
    "\n",
    "def add_distance_to_center(df):\n",
    "    df['d_c'] = ((\n",
    "        (df['x_c'] - df['x'])**np.float32(2) +\n",
    "        (df['y_c'] - df['y'])**np.float32(2) + \n",
    "        (df['z_c'] - df['z'])**np.float32(2)\n",
    "    )**np.float32(0.5))\n",
    "\n",
    "def add_distance_between(df, suffix1, suffix2):\n",
    "    df[f'd_{suffix1}_{suffix2}'] = ((\n",
    "        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n",
    "        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n",
    "        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n",
    "    )**np.float32(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distances(df):\n",
    "    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startswith('x_')])\n",
    "    \n",
    "    for i in range(1, n_atoms):\n",
    "        for vi in range(min(4, i)):\n",
    "            add_distance_between(df, i, vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_n_atoms(base, structures):\n",
    "    dfs = structures['molecule_name'].value_counts().rename('n_atoms').to_frame()\n",
    "    return pd.merge(base, dfs, left_on='molecule_name', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=10):\n",
    "    base, structures = build_type_dataframes(some_csv, structures_csv, coupling_type)\n",
    "    base = add_coordinates(base, structures, 0)\n",
    "    base = add_coordinates(base, structures, 1)\n",
    "    \n",
    "    base = base.drop(['atom_0', 'atom_1'], axis=1)\n",
    "    atoms = base.drop('id', axis=1).copy()\n",
    "    if 'scalar_coupling_constant' in some_csv:\n",
    "        atoms = atoms.drop(['scalar_coupling_constant'], axis=1)\n",
    "        \n",
    "    add_center(atoms)\n",
    "    atoms = atoms.drop(['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'], axis=1)\n",
    "\n",
    "    atoms = merge_all_atoms(atoms, structures)\n",
    "    \n",
    "    add_distance_to_center(atoms)\n",
    "    \n",
    "    atoms = atoms.drop(['x_c', 'y_c', 'z_c', 'atom_index'], axis=1)\n",
    "    atoms.sort_values(['molecule_name', 'atom_index_0', 'atom_index_1', 'd_c'], inplace=True)\n",
    "    atom_groups = atoms.groupby(['molecule_name', 'atom_index_0', 'atom_index_1'])\n",
    "    atoms['num'] = atom_groups.cumcount() + 2\n",
    "    atoms = atoms.drop(['d_c'], axis=1)\n",
    "    atoms = atoms[atoms['num'] < n_atoms]\n",
    "\n",
    "    atoms = atoms.set_index(['molecule_name', 'atom_index_0', 'atom_index_1', 'num']).unstack()\n",
    "    atoms.columns = [f'{col[0]}_{col[1]}' for col in atoms.columns]\n",
    "    atoms = atoms.reset_index()\n",
    "    \n",
    "#     # downcast back to int8\n",
    "    for col in atoms.columns:\n",
    "        if col.startswith('atom_'):\n",
    "            atoms[col] = atoms[col].fillna(0).astype('int8')\n",
    "            \n",
    "#     atoms['molecule_name'] = atoms['molecule_name'].astype('int32')\n",
    "    \n",
    "    full = add_atoms(base, atoms)\n",
    "    add_distances(full)\n",
    "    \n",
    "    full.sort_values('id', inplace=True)\n",
    "    \n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_n_atoms(df, n_atoms, four_start=4):\n",
    "    labels = ['id', 'molecule_name', 'atom_index_1', 'atom_index_0']\n",
    "    for i in range(2, n_atoms):\n",
    "        label = f'atom_{i}'\n",
    "        labels.append(label)\n",
    "\n",
    "    for i in range(n_atoms):\n",
    "        num = min(i, 4) if i < four_start else 4\n",
    "        for j in range(num):\n",
    "            labels.append(f'd_{i}_{j}')\n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        labels.append('scalar_coupling_constant')\n",
    "    return df[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='8' class='' max='8', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8/8 00:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "atoms = structures['atom'].values\n",
    "types_train = train['type'].values\n",
    "types_test = test['type'].values\n",
    "structures['atom'] = structures['atom'].replace(ATOMIC_NUMBERS).astype('int8')\n",
    "fulls_train = []\n",
    "fulls_test  = []\n",
    "for type_ in progress_bar(train['type'].unique()):\n",
    "    full_train = build_couple_dataframe(train, structures, type_, n_atoms=10)\n",
    "    full_test  = build_couple_dataframe(test, structures, type_, n_atoms=10)\n",
    "    full_train = take_n_atoms(full_train, 10)\n",
    "    full_test  = take_n_atoms(full_test, 10)\n",
    "    fulls_train.append(full_train)\n",
    "    fulls_test.append(full_test)\n",
    "    \n",
    "structures['atom'] = atoms\n",
    "train = pd.concat(fulls_train).sort_values(by=['id']) #, axis=0)\n",
    "test  = pd.concat(fulls_test).sort_values(by=['id']) #, axis=0)\n",
    "train['type'] = types_train\n",
    "test['type'] = types_test\n",
    "train = train.fillna(0)\n",
    "test  = test.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "dist-interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dist_interact'] = dist_interact_train.values\n",
    "test['dist_interact'] = dist_interact_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info(df_1,df_2, atom_idx):\n",
    "    df = pd.merge(df_1, df_2, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# structure and ob_charges\n",
    "ob_charge = pd.concat([ob_charge_train, ob_charge_test])\n",
    "merge = pd.merge(ob_charge, structures, how='left',\n",
    "                  left_on  = ['molecule_name', 'atom_index'],\n",
    "                  right_on = ['molecule_name', 'atom_index'])\n",
    "for atom_idx in [0,1]:\n",
    "    train = map_atom_info(train, merge, atom_idx)\n",
    "    train = map_atom_info(train, charge, atom_idx)\n",
    "    train = map_atom_info(train, tensor, atom_idx)\n",
    "    test  = map_atom_info(test,  merge, atom_idx)\n",
    "    \n",
    "    train = train.rename(columns={\n",
    "                                        'atom': f'atom_{atom_idx}',\n",
    "                                        'x': f'x_{atom_idx}',\n",
    "                                        'y': f'y_{atom_idx}',\n",
    "                                        'z': f'z_{atom_idx}',\n",
    "                                        'eem': f'eem_{atom_idx}',\n",
    "                                     'mmff94': f'mmff94_{atom_idx}',\n",
    "                                  'gasteiger': f'gasteiger_{atom_idx}', \n",
    "                                        'qeq': f'qeq_{atom_idx}',\n",
    "                                      'qtpie': f'qtpie_{atom_idx}', \n",
    "                                  'eem2015ha': f'eem2015ha_{atom_idx}', \n",
    "                                  'eem2015hm': f'eem2015hm_{atom_idx}', \n",
    "                                  'eem2015hn': f'eem2015hn_{atom_idx}', \n",
    "                                  'eem2015ba': f'eem2015ba_{atom_idx}', \n",
    "                                  'eem2015bm': f'eem2015bm_{atom_idx}', \n",
    "                                  'eem2015bn': f'eem2015bn_{atom_idx}',\n",
    "                            'mulliken_charge': f'charge_{atom_idx}',\n",
    "                                         'XX': f'XX_{atom_idx}',\n",
    "                                         'YX': f'YX_{atom_idx}',\n",
    "                                         'ZX': f'ZX_{atom_idx}',\n",
    "                                         'XY': f'XY_{atom_idx}',\n",
    "                                         'YY': f'YY_{atom_idx}',\n",
    "                                         'ZY': f'ZY_{atom_idx}',\n",
    "                                         'XZ': f'XZ_{atom_idx}',\n",
    "                                         'YZ': f'YZ_{atom_idx}',\n",
    "                                         'ZZ': f'ZZ_{atom_idx}',})\n",
    "    test = test.rename(columns={\n",
    "                                        'atom': f'atom_{atom_idx}',\n",
    "                                        'x': f'x_{atom_idx}',\n",
    "                                        'y': f'y_{atom_idx}',\n",
    "                                        'z': f'z_{atom_idx}',\n",
    "                                        'eem': f'eem_{atom_idx}',\n",
    "                                     'mmff94': f'mmff94_{atom_idx}',\n",
    "                                  'gasteiger': f'gasteiger_{atom_idx}', \n",
    "                                        'qeq': f'qeq_{atom_idx}', \n",
    "                                      'qtpie': f'qtpie_{atom_idx}', \n",
    "                                  'eem2015ha': f'eem2015ha_{atom_idx}', \n",
    "                                  'eem2015hm': f'eem2015hm_{atom_idx}', \n",
    "                                  'eem2015hn': f'eem2015hn_{atom_idx}', \n",
    "                                  'eem2015ba': f'eem2015ba_{atom_idx}', \n",
    "                                  'eem2015bm': f'eem2015bm_{atom_idx}', \n",
    "                                  'eem2015bn': f'eem2015bn_{atom_idx}'})\n",
    "#     test  =  test.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "#                                         'x': f'x_{atom_idx}',\n",
    "#                                         'y': f'y_{atom_idx}',\n",
    "#                                         'z': f'z_{atom_idx}'})\n",
    "\n",
    "# ob_charges\n",
    "# train = map_atom_info(train, ob_charge_train, 0)\n",
    "# test  = map_atom_info(test,  ob_charge_test,  0)\n",
    "# train = map_atom_info(train, ob_charge_train, 1)\n",
    "# test  = map_atom_info(test,  ob_charge_test,  1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "type0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_type0(df):\n",
    "    df['type_0'] = df['type'].apply(lambda x : x[0])\n",
    "    return df\n",
    "# train['type_0'] = train['type'].apply(lambda x: x[0])\n",
    "# test['type_0'] = test['type'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances(df):\n",
    "    df_p_0 = df[['x_0', 'y_0', 'z_0']].values\n",
    "    df_p_1 = df[['x_1', 'y_1', 'z_1']].values\n",
    "    \n",
    "    df['dist'] = np.linalg.norm(df_p_0 - df_p_1, axis=1)\n",
    "    df['dist_x'] = (df['x_0'] - df['x_1']) ** 2\n",
    "    df['dist_y'] = (df['y_0'] - df['y_1']) ** 2\n",
    "    df['dist_z'] = (df['z_0'] - df['z_1']) ** 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "# train = distances(train)\n",
    "# test  = distances(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "統計量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df['molecule_couples'] = df.groupby('molecule_name')['id'].transform('count')\n",
    "    df['molecule_dist_mean'] = df.groupby('molecule_name')['dist'].transform('mean')\n",
    "    df['molecule_dist_min'] = df.groupby('molecule_name')['dist'].transform('min')\n",
    "    df['molecule_dist_max'] = df.groupby('molecule_name')['dist'].transform('max')\n",
    "    df['atom_0_couples_count'] = df.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\n",
    "    df['atom_1_couples_count'] = df.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n",
    "    df[f'molecule_atom_index_0_x_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['x_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_y_1_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_y_1_mean_diff'] = df[f'molecule_atom_index_0_y_1_mean'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_mean_div'] = df[f'molecule_atom_index_0_y_1_mean'] / df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_max'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('max')\n",
    "    df[f'molecule_atom_index_0_y_1_max_diff'] = df[f'molecule_atom_index_0_y_1_max'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_z_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['z_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_dist_mean_diff'] = df[f'molecule_atom_index_0_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_mean_div'] = df[f'molecule_atom_index_0_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_0_dist_max_diff'] = df[f'molecule_atom_index_0_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max_div'] = df[f'molecule_atom_index_0_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_0_dist_min_diff'] = df[f'molecule_atom_index_0_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min_div'] = df[f'molecule_atom_index_0_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_std_diff'] = df[f'molecule_atom_index_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std_div'] = df[f'molecule_atom_index_0_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_1_dist_mean_diff'] = df[f'molecule_atom_index_1_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean_div'] = df[f'molecule_atom_index_1_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_1_dist_max_diff'] = df[f'molecule_atom_index_1_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max_div'] = df[f'molecule_atom_index_1_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_1_dist_min_diff'] = df[f'molecule_atom_index_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min_div'] = df[f'molecule_atom_index_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_1_dist_std_diff'] = df[f'molecule_atom_index_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std_div'] = df[f'molecule_atom_index_1_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_mean'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_1_dist_min'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_1_dist_min_diff'] = df[f'molecule_atom_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_1_dist_min_div'] = df[f'molecule_atom_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_std'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_1_dist_std_diff'] = df[f'molecule_atom_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_0_dist_std'] = df.groupby(['molecule_name', 'type_0'])['dist'].transform('std')\n",
    "    df[f'molecule_type_0_dist_std_diff'] = df[f'molecule_type_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean'] = df.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n",
    "    df[f'molecule_type_dist_mean_diff'] = df[f'molecule_type_dist_mean'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean_div'] = df[f'molecule_type_dist_mean'] / df['dist']\n",
    "    df[f'molecule_type_dist_max'] = df.groupby(['molecule_name', 'type'])['dist'].transform('max')\n",
    "    df[f'molecule_type_dist_min'] = df.groupby(['molecule_name', 'type'])['dist'].transform('min')\n",
    "    df[f'molecule_type_dist_std'] = df.groupby(['molecule_name', 'type'])['dist'].transform('std')\n",
    "    df[f'molecule_type_dist_std_diff'] = df[f'molecule_type_dist_std'] - df['dist']\n",
    "    # fc\n",
    "    df[f'molecule_type_fc_max'] = df.groupby(['molecule_name', 'type'])['fc'].transform('max')\n",
    "    df[f'molecule_type_fc_min'] = df.groupby(['molecule_name', 'type'])['fc'].transform('min')\n",
    "    df[f'molecule_type_fc_std'] = df.groupby(['molecule_name', 'type'])['fc'].transform('std')\n",
    "    df[f'molecule_type_fc_std_diff'] = df[f'molecule_type_fc_std'] - df['fc']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "angle features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info(df_1,df_2, atom_idx):\n",
    "    df = pd.merge(df_1, df_2, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_closest(df):\n",
    "    df_temp=df.loc[:,[\"molecule_name\",\"atom_index_0\",\"atom_index_1\",\"dist\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n",
    "    df_temp_=df_temp.copy()\n",
    "    df_temp_= df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
    "                                       'atom_index_1': 'atom_index_0',\n",
    "                                       'x_0': 'x_1',\n",
    "                                       'y_0': 'y_1',\n",
    "                                       'z_0': 'z_1',\n",
    "                                       'x_1': 'x_0',\n",
    "                                       'y_1': 'y_0',\n",
    "                                       'z_1': 'z_0'})\n",
    "    df_temp=pd.concat(objs=[df_temp,df_temp_],axis=0)\n",
    "\n",
    "    df_temp[\"min_distance\"]=df_temp.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df_temp= df_temp[df_temp[\"min_distance\"]==df_temp[\"dist\"]]\n",
    "\n",
    "    df_temp=df_temp.drop(['x_0','y_0','z_0','min_distance', 'dist'], axis=1)\n",
    "    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                     'atom_index_1': 'atom_index_closest',\n",
    "                                     'distance': 'distance_closest',\n",
    "                                     'x_1': 'x_closest',\n",
    "                                     'y_1': 'y_closest',\n",
    "                                     'z_1': 'z_closest'})\n",
    "\n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df,df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
    "                                        'distance_closest': f'distance_closest_{atom_idx}',\n",
    "                                        'x_closest': f'x_closest_{atom_idx}',\n",
    "                                        'y_closest': f'y_closest_{atom_idx}',\n",
    "                                        'z_closest': f'z_closest_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "def add_cos_features(df):\n",
    "    df[\"distance_0\"]=((df['x_0']-df['x_closest_0'])**2+(df['y_0']-df['y_closest_0'])**2+(df['z_0']-df['z_closest_0'])**2)**(1/2)\n",
    "    df[\"distance_1\"]=((df['x_1']-df['x_closest_1'])**2+(df['y_1']-df['y_closest_1'])**2+(df['z_1']-df['z_closest_1'])**2)**(1/2)\n",
    "    df[\"vec_0_x\"]=(df['x_0']-df['x_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_0_y\"]=(df['y_0']-df['y_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_0_z\"]=(df['z_0']-df['z_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_1_x\"]=(df['x_1']-df['x_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_1_y\"]=(df['y_1']-df['y_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_1_z\"]=(df['z_1']-df['z_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_x\"]=(df['x_1']-df['x_0'])/df[\"dist\"]\n",
    "    df[\"vec_y\"]=(df['y_1']-df['y_0'])/df[\"dist\"]\n",
    "    df[\"vec_z\"]=(df['z_1']-df['z_0'])/df[\"dist\"]\n",
    "    df[\"cos_0_1\"]=df[\"vec_0_x\"]*df[\"vec_1_x\"]+df[\"vec_0_y\"]*df[\"vec_1_y\"]+df[\"vec_0_z\"]*df[\"vec_1_z\"]\n",
    "    df[\"cos_0\"]=df[\"vec_0_x\"]*df[\"vec_x\"]+df[\"vec_0_y\"]*df[\"vec_y\"]+df[\"vec_0_z\"]*df[\"vec_z\"]\n",
    "    df[\"cos_1\"]=df[\"vec_1_x\"]*df[\"vec_x\"]+df[\"vec_1_y\"]*df[\"vec_y\"]+df[\"vec_1_z\"]*df[\"vec_z\"]\n",
    "    df=df.drop(['vec_0_x','vec_0_y','vec_0_z','vec_1_x','vec_1_y','vec_1_z','vec_x','vec_y','vec_z'], axis=1)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add fc\n",
      "10000 10000\n",
      "type0\n",
      "10000 10000\n",
      "distances\n",
      "10000 10000\n",
      "create_featueres\n",
      "10000 10000\n",
      "create_closest\n",
      "10000 10000\n",
      "add_cos_features\n",
      "10000 10000\n",
      "CPU times: user 580 ms, sys: 24 ms, total: 604 ms\n",
      "Wall time: 601 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('add fc')\n",
    "print(len(train), len(test))\n",
    "train['fc'] = fc_train.values\n",
    "test['fc']  = fc_test.values\n",
    "\n",
    "print('type0')\n",
    "print(len(train), len(test))\n",
    "train = create_type0(train)\n",
    "test  = create_type0(test)\n",
    "\n",
    "print('distances')\n",
    "print(len(train), len(test))\n",
    "train = distances(train)\n",
    "test  = distances(test)\n",
    "\n",
    "print('create_featueres')\n",
    "print(len(train), len(test))\n",
    "train = create_features(train)\n",
    "test  = create_features(test)\n",
    "\n",
    "print('create_closest')\n",
    "print(len(train), len(test))\n",
    "train = create_closest(train)\n",
    "test  = create_closest(test)\n",
    "\n",
    "print('add_cos_features')\n",
    "print(len(train), len(test))\n",
    "train = add_cos_features(train)\n",
    "test  = add_cos_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "nanがある特徴量を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['molecule_atom_index_0_x_1_std',\n",
       "       'molecule_atom_index_0_y_1_mean_div',\n",
       "       'molecule_atom_index_0_y_1_std', 'molecule_atom_index_0_z_1_std',\n",
       "       'molecule_atom_index_0_dist_std',\n",
       "       'molecule_atom_index_0_dist_std_diff',\n",
       "       'molecule_atom_index_0_dist_std_div',\n",
       "       'molecule_atom_index_1_dist_std',\n",
       "       'molecule_atom_index_1_dist_std_diff',\n",
       "       'molecule_atom_index_1_dist_std_div', 'molecule_atom_1_dist_std',\n",
       "       'molecule_atom_1_dist_std_diff', 'molecule_type_0_dist_std',\n",
       "       'molecule_type_0_dist_std_diff', 'molecule_type_dist_std',\n",
       "       'molecule_type_dist_std_diff', 'molecule_type_fc_std',\n",
       "       'molecule_type_fc_std_diff'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nan がある特徴量\n",
    "drop_feats = train.columns[train.isnull().sum(axis=0) != 0].values\n",
    "drop_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(drop_feats, axis=1)\n",
    "test  = test.drop(drop_feats, axis=1)\n",
    "\n",
    "assert sum(train.isnull().sum(axis=0))==0, f'train に nan があります。'\n",
    "assert sum(test.isnull().sum(axis=0))==0,  f'test に nan があります。'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "カテゴリカル特徴量 と 数値特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='153' class='' max='153', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [153/153 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# inf がある特徴量を削除\n",
    "df = train\n",
    "for feat in progress_bar(df.columns):\n",
    "    logi = (df[feat]==np.inf)\n",
    "    if sum(logi) >= 1:\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atom_index_1\n",
      "atom_index_0\n",
      "molecule_couples\n",
      "atom_0_couples_count\n",
      "atom_1_couples_count\n",
      "atom_index_closest_0\n",
      "atom_index_closest_1\n"
     ]
    }
   ],
   "source": [
    "# int 型の列挙\n",
    "for feat in train.columns:\n",
    "    if train[feat].dtypes == np.dtype('int64'):\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カテゴリカル: ['atom_1', 'type_0', 'type']\n",
      "数値:        ['molecule_dist_max', 'atom_index_0', 'eem2015bm_1', 'd_9_2', 'eem2015bn_0', 'atom_3', 'd_9_0', 'dist_interact', 'XX_0', 'd_5_2', 'x_closest_0', 'eem_0', 'd_4_1', 'eem2015ha_0', 'molecule_type_dist_max', 'molecule_atom_index_0_dist_max', 'd_6_2', 'z_closest_0', 'molecule_atom_index_0_y_1_mean', 'eem2015bm_0', 'd_3_0', 'molecule_atom_index_1_dist_mean_div', 'd_6_1', 'd_8_0', 'd_6_3', 'atom_7', 'charge_1', 'ZY_1', 'd_4_0', 'z_0', 'atom_8', 'eem2015hm_1', 'd_3_2', 'dist_x', 'd_7_2', 'YX_1', 'molecule_dist_mean', 'y_0', 'molecule_atom_index_0_dist_min_div', 'molecule_atom_index_1_dist_mean', 'molecule_dist_min', 'fc', 'molecule_couples', 'd_5_0', 'z_1', 'x_1', 'distance_1', 'd_5_1', 'd_3_1', 'YZ_0', 'XX_1', 'molecule_atom_index_1_dist_mean_diff', 'atom_5', 'eem2015ha_1', 'eem2015hm_0', 'ZX_1', 'ZY_0', 'molecule_atom_1_dist_min_diff', 'ZX_0', 'ZZ_1', 'molecule_atom_1_dist_mean', 'XY_0', 'molecule_type_fc_min', 'z_closest_1', 'distance_0', 'molecule_atom_index_0_dist_mean_div', 'd_2_0', 'molecule_atom_index_1_dist_max', 'mmff94_1', 'd_5_3', 'XY_1', 'd_7_1', 'cos_0', 'molecule_atom_index_0_y_1_mean_diff', 'molecule_atom_index_1_dist_min_div', 'molecule_type_dist_mean_diff', 'd_9_3', 'd_8_1', 'eem2015hn_1', 'atom_2', 'YX_0', 'molecule_atom_index_0_dist_max_diff', 'ZZ_0', 'atom_6', 'molecule_type_fc_max', 'gasteiger_1', 'molecule_type_dist_min', 'cos_1', 'd_7_3', 'eem2015ba_0', 'molecule_atom_index_1_dist_max_div', 'x_0', 'molecule_atom_index_0_y_1_max', 'eem2015hn_0', 'eem2015bn_1', 'x_closest_1', 'd_4_2', 'd_1_0', 'molecule_type_dist_mean_div', 'molecule_atom_index_0_dist_max_div', 'qtpie_0', 'XZ_0', 'y_closest_1', 'molecule_atom_index_1_dist_max_diff', 'molecule_atom_index_0_dist_mean', 'molecule_atom_1_dist_min', 'YY_1', 'd_7_0', 'molecule_atom_index_1_dist_min', 'qeq_1', 'XZ_1', 'dist_z', 'd_8_3', 'mmff94_0', 'molecule_atom_index_1_dist_min_diff', 'molecule_atom_index_0_y_1_max_diff', 'y_closest_0', 'd_9_1', 'molecule_atom_index_0_dist_min_diff', 'dist', 'qeq_0', 'atom_index_1', 'dist_y', 'qtpie_1', 'YZ_1', 'gasteiger_0', 'molecule_atom_index_0_dist_mean_diff', 'eem2015ba_1', 'molecule_atom_1_dist_min_div', 'molecule_atom_index_0_dist_min', 'atom_4', 'cos_0_1', 'molecule_type_dist_mean', 'YY_0', 'd_4_3', 'y_1', 'd_2_1', 'charge_0', 'd_6_0', 'eem_1', 'atom_1_couples_count', 'atom_9', 'd_8_2']\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['atom_1','type_0','type']\n",
    "num_cols = list(set(train.columns) - set(cat_cols) - set([\"scalar_coupling_constant\", 'molecule_name', 'id', 'atom_0',\n",
    "                                                          'atom_index_closest_0', 'atom_index_closest_1', 'atom_0_couples_count', 'atom_']))\n",
    "print(f'カテゴリカル: {cat_cols}')\n",
    "print(f'数値:        {num_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "<br>\n",
    "LabelEncode\n",
    "\n",
    "- `type`   = {2JHC, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['type']:\n",
    "    if f in train.columns:\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[f].values) + list(test[f].values))\n",
    "        train[f] = lbl.transform(list(train[f].values))\n",
    "        test[f] = lbl.transform(list(test[f].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "**show features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_2</th>\n",
       "      <th>atom_3</th>\n",
       "      <th>atom_4</th>\n",
       "      <th>atom_5</th>\n",
       "      <th>atom_6</th>\n",
       "      <th>atom_7</th>\n",
       "      <th>atom_8</th>\n",
       "      <th>atom_9</th>\n",
       "      <th>d_1_0</th>\n",
       "      <th>d_2_0</th>\n",
       "      <th>d_2_1</th>\n",
       "      <th>d_3_0</th>\n",
       "      <th>d_3_1</th>\n",
       "      <th>d_3_2</th>\n",
       "      <th>d_4_0</th>\n",
       "      <th>d_4_1</th>\n",
       "      <th>d_4_2</th>\n",
       "      <th>d_4_3</th>\n",
       "      <th>d_5_0</th>\n",
       "      <th>d_5_1</th>\n",
       "      <th>d_5_2</th>\n",
       "      <th>d_5_3</th>\n",
       "      <th>d_6_0</th>\n",
       "      <th>d_6_1</th>\n",
       "      <th>d_6_2</th>\n",
       "      <th>d_6_3</th>\n",
       "      <th>d_7_0</th>\n",
       "      <th>d_7_1</th>\n",
       "      <th>d_7_2</th>\n",
       "      <th>d_7_3</th>\n",
       "      <th>d_8_0</th>\n",
       "      <th>d_8_1</th>\n",
       "      <th>d_8_2</th>\n",
       "      <th>d_8_3</th>\n",
       "      <th>d_9_0</th>\n",
       "      <th>d_9_1</th>\n",
       "      <th>d_9_2</th>\n",
       "      <th>d_9_3</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "      <th>type</th>\n",
       "      <th>dist_interact</th>\n",
       "      <th>eem_0</th>\n",
       "      <th>mmff94_0</th>\n",
       "      <th>gasteiger_0</th>\n",
       "      <th>qeq_0</th>\n",
       "      <th>qtpie_0</th>\n",
       "      <th>eem2015ha_0</th>\n",
       "      <th>eem2015hm_0</th>\n",
       "      <th>eem2015hn_0</th>\n",
       "      <th>eem2015ba_0</th>\n",
       "      <th>eem2015bm_0</th>\n",
       "      <th>eem2015bn_0</th>\n",
       "      <th>atom_0</th>\n",
       "      <th>x_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>z_0</th>\n",
       "      <th>charge_0</th>\n",
       "      <th>XX_0</th>\n",
       "      <th>YX_0</th>\n",
       "      <th>ZX_0</th>\n",
       "      <th>XY_0</th>\n",
       "      <th>YY_0</th>\n",
       "      <th>ZY_0</th>\n",
       "      <th>XZ_0</th>\n",
       "      <th>YZ_0</th>\n",
       "      <th>ZZ_0</th>\n",
       "      <th>eem_1</th>\n",
       "      <th>mmff94_1</th>\n",
       "      <th>gasteiger_1</th>\n",
       "      <th>qeq_1</th>\n",
       "      <th>qtpie_1</th>\n",
       "      <th>eem2015ha_1</th>\n",
       "      <th>eem2015hm_1</th>\n",
       "      <th>eem2015hn_1</th>\n",
       "      <th>eem2015ba_1</th>\n",
       "      <th>eem2015bm_1</th>\n",
       "      <th>eem2015bn_1</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>charge_1</th>\n",
       "      <th>XX_1</th>\n",
       "      <th>YX_1</th>\n",
       "      <th>ZX_1</th>\n",
       "      <th>XY_1</th>\n",
       "      <th>YY_1</th>\n",
       "      <th>ZY_1</th>\n",
       "      <th>XZ_1</th>\n",
       "      <th>YZ_1</th>\n",
       "      <th>ZZ_1</th>\n",
       "      <th>fc</th>\n",
       "      <th>type_0</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_x</th>\n",
       "      <th>dist_y</th>\n",
       "      <th>dist_z</th>\n",
       "      <th>molecule_couples</th>\n",
       "      <th>molecule_dist_mean</th>\n",
       "      <th>molecule_dist_min</th>\n",
       "      <th>molecule_dist_max</th>\n",
       "      <th>atom_0_couples_count</th>\n",
       "      <th>atom_1_couples_count</th>\n",
       "      <th>molecule_atom_index_0_y_1_mean</th>\n",
       "      <th>molecule_atom_index_0_y_1_mean_diff</th>\n",
       "      <th>molecule_atom_index_0_y_1_max</th>\n",
       "      <th>molecule_atom_index_0_y_1_max_diff</th>\n",
       "      <th>molecule_atom_index_0_dist_mean</th>\n",
       "      <th>molecule_atom_index_0_dist_mean_diff</th>\n",
       "      <th>molecule_atom_index_0_dist_mean_div</th>\n",
       "      <th>molecule_atom_index_0_dist_max</th>\n",
       "      <th>molecule_atom_index_0_dist_max_diff</th>\n",
       "      <th>molecule_atom_index_0_dist_max_div</th>\n",
       "      <th>molecule_atom_index_0_dist_min</th>\n",
       "      <th>molecule_atom_index_0_dist_min_diff</th>\n",
       "      <th>molecule_atom_index_0_dist_min_div</th>\n",
       "      <th>molecule_atom_index_1_dist_mean</th>\n",
       "      <th>molecule_atom_index_1_dist_mean_diff</th>\n",
       "      <th>molecule_atom_index_1_dist_mean_div</th>\n",
       "      <th>molecule_atom_index_1_dist_max</th>\n",
       "      <th>molecule_atom_index_1_dist_max_diff</th>\n",
       "      <th>molecule_atom_index_1_dist_max_div</th>\n",
       "      <th>molecule_atom_index_1_dist_min</th>\n",
       "      <th>molecule_atom_index_1_dist_min_diff</th>\n",
       "      <th>molecule_atom_index_1_dist_min_div</th>\n",
       "      <th>molecule_atom_1_dist_mean</th>\n",
       "      <th>molecule_atom_1_dist_min</th>\n",
       "      <th>molecule_atom_1_dist_min_diff</th>\n",
       "      <th>molecule_atom_1_dist_min_div</th>\n",
       "      <th>molecule_type_dist_mean</th>\n",
       "      <th>molecule_type_dist_mean_diff</th>\n",
       "      <th>molecule_type_dist_mean_div</th>\n",
       "      <th>molecule_type_dist_max</th>\n",
       "      <th>molecule_type_dist_min</th>\n",
       "      <th>molecule_type_fc_max</th>\n",
       "      <th>molecule_type_fc_min</th>\n",
       "      <th>atom_index_closest_0</th>\n",
       "      <th>x_closest_0</th>\n",
       "      <th>y_closest_0</th>\n",
       "      <th>z_closest_0</th>\n",
       "      <th>atom_index_closest_1</th>\n",
       "      <th>x_closest_1</th>\n",
       "      <th>y_closest_1</th>\n",
       "      <th>z_closest_1</th>\n",
       "      <th>distance_0</th>\n",
       "      <th>distance_1</th>\n",
       "      <th>cos_0_1</th>\n",
       "      <th>cos_0</th>\n",
       "      <th>cos_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>1.783147</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>1.783158</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>1.091948</td>\n",
       "      <td>1.783148</td>\n",
       "      <td>1.783148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.8076</td>\n",
       "      <td>0</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>0.161131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019399</td>\n",
       "      <td>-0.812772</td>\n",
       "      <td>0.773439</td>\n",
       "      <td>-0.003651</td>\n",
       "      <td>0.203254</td>\n",
       "      <td>0.196234</td>\n",
       "      <td>0.016837</td>\n",
       "      <td>0.201583</td>\n",
       "      <td>0.212813</td>\n",
       "      <td>H</td>\n",
       "      <td>0.00215</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.133921</td>\n",
       "      <td>31.341</td>\n",
       "      <td>-1.2317</td>\n",
       "      <td>4.0544</td>\n",
       "      <td>-1.2317</td>\n",
       "      <td>28.9546</td>\n",
       "      <td>-1.7173</td>\n",
       "      <td>4.0546</td>\n",
       "      <td>-1.7173</td>\n",
       "      <td>34.0861</td>\n",
       "      <td>-0.644531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.077596</td>\n",
       "      <td>3.251140</td>\n",
       "      <td>-3.093807</td>\n",
       "      <td>0.014606</td>\n",
       "      <td>-0.813021</td>\n",
       "      <td>-0.784944</td>\n",
       "      <td>-0.067349</td>\n",
       "      <td>-0.806339</td>\n",
       "      <td>-0.851258</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>-0.535689</td>\n",
       "      <td>195.3150</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>195.3170</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>195.3170</td>\n",
       "      <td>83.534069</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>1.192105</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>10</td>\n",
       "      <td>1.506668</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>1.783158</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.358754</td>\n",
       "      <td>0.272949</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.377947</td>\n",
       "      <td>1.610344</td>\n",
       "      <td>0.518391</td>\n",
       "      <td>1.474738</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>0.691204</td>\n",
       "      <td>1.632998</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.09195</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>1.091950</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>1.091950</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>87.113102</td>\n",
       "      <td>83.162207</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>0.333335</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.333335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>1.783148</td>\n",
       "      <td>1.091948</td>\n",
       "      <td>1.783147</td>\n",
       "      <td>1.783158</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>1.783148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.2570</td>\n",
       "      <td>3</td>\n",
       "      <td>2.183905</td>\n",
       "      <td>0.161131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019399</td>\n",
       "      <td>-0.812772</td>\n",
       "      <td>0.773439</td>\n",
       "      <td>-0.003651</td>\n",
       "      <td>0.203254</td>\n",
       "      <td>0.196234</td>\n",
       "      <td>0.016837</td>\n",
       "      <td>0.201583</td>\n",
       "      <td>0.212813</td>\n",
       "      <td>H</td>\n",
       "      <td>0.00215</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.133921</td>\n",
       "      <td>31.341</td>\n",
       "      <td>-1.2317</td>\n",
       "      <td>4.0544</td>\n",
       "      <td>-1.2317</td>\n",
       "      <td>28.9546</td>\n",
       "      <td>-1.7173</td>\n",
       "      <td>4.0546</td>\n",
       "      <td>-1.7173</td>\n",
       "      <td>34.0861</td>\n",
       "      <td>0.161132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019399</td>\n",
       "      <td>-0.812776</td>\n",
       "      <td>0.773442</td>\n",
       "      <td>-0.003651</td>\n",
       "      <td>0.203254</td>\n",
       "      <td>0.196235</td>\n",
       "      <td>0.016837</td>\n",
       "      <td>0.201584</td>\n",
       "      <td>0.212813</td>\n",
       "      <td>H</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.133922</td>\n",
       "      <td>31.5814</td>\n",
       "      <td>1.2173</td>\n",
       "      <td>-4.1474</td>\n",
       "      <td>1.2173</td>\n",
       "      <td>28.9036</td>\n",
       "      <td>-1.6036</td>\n",
       "      <td>-4.1476</td>\n",
       "      <td>-1.6036</td>\n",
       "      <td>33.8967</td>\n",
       "      <td>-11.692044</td>\n",
       "      <td>2</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>1.019253</td>\n",
       "      <td>2.160261</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>10</td>\n",
       "      <td>1.506668</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>1.783158</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.358754</td>\n",
       "      <td>-0.104998</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.610344</td>\n",
       "      <td>-0.172776</td>\n",
       "      <td>0.903105</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>1.000021</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>-0.691167</td>\n",
       "      <td>0.612383</td>\n",
       "      <td>1.78312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.783146</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.783146</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.000015</td>\n",
       "      <td>1.783158</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>-10.558171</td>\n",
       "      <td>-11.692044</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>-0.333287</td>\n",
       "      <td>-0.816483</td>\n",
       "      <td>0.816482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     molecule_name  atom_index_1  atom_index_0  atom_2  atom_3  atom_4  \\\n",
       "0   0  dsgdb9nsd_000001             0             1       1       1       1   \n",
       "1   1  dsgdb9nsd_000001             2             1       6       1       1   \n",
       "\n",
       "   atom_5  atom_6  atom_7  atom_8  atom_9     d_1_0     d_2_0     d_2_1  \\\n",
       "0       0       0       0       0       0  1.091953  1.783120  1.091952   \n",
       "1       0       0       0       0       0  1.783120  1.091953  1.091952   \n",
       "\n",
       "      d_3_0     d_3_1     d_3_2     d_4_0     d_4_1     d_4_2     d_4_3  \\\n",
       "0  1.783147  1.091946  1.783158  1.783157  1.091948  1.783148  1.783148   \n",
       "1  1.783157  1.783148  1.091948  1.783147  1.783158  1.091946  1.783148   \n",
       "\n",
       "   d_5_0  d_5_1  d_5_2  d_5_3  d_6_0  d_6_1  d_6_2  d_6_3  d_7_0  d_7_1  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   d_7_2  d_7_3  d_8_0  d_8_1  d_8_2  d_8_3  d_9_0  d_9_1  d_9_2  d_9_3  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   scalar_coupling_constant  type  dist_interact     eem_0  mmff94_0  \\\n",
       "0                   84.8076     0       1.091953  0.161131       0.0   \n",
       "1                  -11.2570     3       2.183905  0.161131       0.0   \n",
       "\n",
       "   gasteiger_0     qeq_0   qtpie_0  eem2015ha_0  eem2015hm_0  eem2015hn_0  \\\n",
       "0     0.019399 -0.812772  0.773439    -0.003651     0.203254     0.196234   \n",
       "1     0.019399 -0.812772  0.773439    -0.003651     0.203254     0.196234   \n",
       "\n",
       "   eem2015ba_0  eem2015bm_0  eem2015bn_0 atom_0      x_0       y_0       z_0  \\\n",
       "0     0.016837     0.201583     0.212813      H  0.00215 -0.006031  0.001976   \n",
       "1     0.016837     0.201583     0.212813      H  0.00215 -0.006031  0.001976   \n",
       "\n",
       "   charge_0    XX_0    YX_0    ZX_0    XY_0     YY_0    ZY_0    XZ_0    YZ_0  \\\n",
       "0  0.133921  31.341 -1.2317  4.0544 -1.2317  28.9546 -1.7173  4.0546 -1.7173   \n",
       "1  0.133921  31.341 -1.2317  4.0544 -1.2317  28.9546 -1.7173  4.0546 -1.7173   \n",
       "\n",
       "      ZZ_0     eem_1  mmff94_1  gasteiger_1     qeq_1   qtpie_1  eem2015ha_1  \\\n",
       "0  34.0861 -0.644531       0.0    -0.077596  3.251140 -3.093807     0.014606   \n",
       "1  34.0861  0.161132       0.0     0.019399 -0.812776  0.773442    -0.003651   \n",
       "\n",
       "   eem2015hm_1  eem2015hn_1  eem2015ba_1  eem2015bm_1  eem2015bn_1 atom_1  \\\n",
       "0    -0.813021    -0.784944    -0.067349    -0.806339    -0.851258      C   \n",
       "1     0.203254     0.196235     0.016837     0.201584     0.212813      H   \n",
       "\n",
       "        x_1       y_1       z_1  charge_1      XX_1    YX_1    ZX_1    XY_1  \\\n",
       "0 -0.012698  1.085804  0.008001 -0.535689  195.3150  0.0000 -0.0001  0.0000   \n",
       "1  1.011731  1.463751  0.000277  0.133922   31.5814  1.2173 -4.1474  1.2173   \n",
       "\n",
       "       YY_1    ZY_1    XZ_1    YZ_1      ZZ_1         fc type_0      dist  \\\n",
       "0  195.3170  0.0007 -0.0001  0.0007  195.3170  83.534069      1  1.091953   \n",
       "1   28.9036 -1.6036 -4.1476 -1.6036   33.8967 -11.692044      2  1.783120   \n",
       "\n",
       "     dist_x    dist_y    dist_z  molecule_couples  molecule_dist_mean  \\\n",
       "0  0.000220  1.192105  0.000036                10            1.506668   \n",
       "1  1.019253  2.160261  0.000003                10            1.506668   \n",
       "\n",
       "   molecule_dist_min  molecule_dist_max  atom_0_couples_count  \\\n",
       "0           1.091946           1.783158                     4   \n",
       "1           1.091946           1.783158                     4   \n",
       "\n",
       "   atom_1_couples_count  molecule_atom_index_0_y_1_mean  \\\n",
       "0                     4                        1.358754   \n",
       "1                     1                        1.358754   \n",
       "\n",
       "   molecule_atom_index_0_y_1_mean_diff  molecule_atom_index_0_y_1_max  \\\n",
       "0                             0.272949                       1.463751   \n",
       "1                            -0.104998                       1.463751   \n",
       "\n",
       "   molecule_atom_index_0_y_1_max_diff  molecule_atom_index_0_dist_mean  \\\n",
       "0                            0.377947                         1.610344   \n",
       "1                            0.000000                         1.610344   \n",
       "\n",
       "   molecule_atom_index_0_dist_mean_diff  molecule_atom_index_0_dist_mean_div  \\\n",
       "0                              0.518391                             1.474738   \n",
       "1                             -0.172776                             0.903105   \n",
       "\n",
       "   molecule_atom_index_0_dist_max  molecule_atom_index_0_dist_max_diff  \\\n",
       "0                        1.783157                             0.691204   \n",
       "1                        1.783157                             0.000037   \n",
       "\n",
       "   molecule_atom_index_0_dist_max_div  molecule_atom_index_0_dist_min  \\\n",
       "0                            1.632998                        1.091953   \n",
       "1                            1.000021                        1.091953   \n",
       "\n",
       "   molecule_atom_index_0_dist_min_diff  molecule_atom_index_0_dist_min_div  \\\n",
       "0                             0.000000                            1.000000   \n",
       "1                            -0.691167                            0.612383   \n",
       "\n",
       "   molecule_atom_index_1_dist_mean  molecule_atom_index_1_dist_mean_diff  \\\n",
       "0                          1.09195                             -0.000003   \n",
       "1                          1.78312                              0.000000   \n",
       "\n",
       "   molecule_atom_index_1_dist_mean_div  molecule_atom_index_1_dist_max  \\\n",
       "0                             0.999997                        1.091953   \n",
       "1                             1.000000                        1.783120   \n",
       "\n",
       "   molecule_atom_index_1_dist_max_diff  molecule_atom_index_1_dist_max_div  \\\n",
       "0                                  0.0                                 1.0   \n",
       "1                                  0.0                                 1.0   \n",
       "\n",
       "   molecule_atom_index_1_dist_min  molecule_atom_index_1_dist_min_diff  \\\n",
       "0                        1.091946                            -0.000007   \n",
       "1                        1.783120                             0.000000   \n",
       "\n",
       "   molecule_atom_index_1_dist_min_div  molecule_atom_1_dist_mean  \\\n",
       "0                            0.999994                   1.091950   \n",
       "1                            1.000000                   1.783146   \n",
       "\n",
       "   molecule_atom_1_dist_min  molecule_atom_1_dist_min_diff  \\\n",
       "0                  1.091946                      -0.000007   \n",
       "1                  1.783120                       0.000000   \n",
       "\n",
       "   molecule_atom_1_dist_min_div  molecule_type_dist_mean  \\\n",
       "0                      0.999994                 1.091950   \n",
       "1                      1.000000                 1.783146   \n",
       "\n",
       "   molecule_type_dist_mean_diff  molecule_type_dist_mean_div  \\\n",
       "0                     -0.000003                     0.999997   \n",
       "1                      0.000027                     1.000015   \n",
       "\n",
       "   molecule_type_dist_max  molecule_type_dist_min  molecule_type_fc_max  \\\n",
       "0                1.091953                1.091946             87.113102   \n",
       "1                1.783158                1.783120            -10.558171   \n",
       "\n",
       "   molecule_type_fc_min  atom_index_closest_0  x_closest_0  y_closest_0  \\\n",
       "0             83.162207                     0    -0.012698     1.085804   \n",
       "1            -11.692044                     0    -0.012698     1.085804   \n",
       "\n",
       "   z_closest_0  atom_index_closest_1  x_closest_1  y_closest_1  z_closest_1  \\\n",
       "0     0.008001                     3    -0.540815     1.447527    -0.876644   \n",
       "1     0.008001                     0    -0.012698     1.085804     0.008001   \n",
       "\n",
       "   distance_0  distance_1   cos_0_1     cos_0     cos_1  \n",
       "0    1.091953    1.091946  0.333335 -1.000000 -0.333335  \n",
       "1    1.091953    1.091952 -0.333287 -0.816483  0.816482  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "Index(['id', 'molecule_name', 'atom_index_1', 'atom_index_0', 'atom_2',\n",
      "       'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7',\n",
      "       ...\n",
      "       'z_closest_0', 'atom_index_closest_1', 'x_closest_1', 'y_closest_1',\n",
      "       'z_closest_1', 'distance_0', 'distance_1', 'cos_0_1', 'cos_0', 'cos_1'],\n",
      "      dtype='object', length=153)\n"
     ]
    }
   ],
   "source": [
    "print(len(train.columns))\n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  8.70 Mb (5.8% reduction)\n",
      "Mem. usage decreased to  8.70 Mb (5.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "y  = train['scalar_coupling_constant']\n",
    "y1 = train[[\"charge_0\",\"charge_1\"]]\n",
    "y2 = train[[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]]\n",
    "y3 = train[[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]]\n",
    "train = train.drop(['id', 'molecule_name', 'atom_0', 'scalar_coupling_constant', 'atom_1', 'type_0'], axis=1)\n",
    "train = train.drop([\"charge_0\",\"charge_1\"], axis=1)\n",
    "train = train.drop([\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"], axis=1)\n",
    "train = train.drop([\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"], axis=1)\n",
    "test  =  test.drop(['id', 'molecule_name', 'atom_0', 'atom_1', 'type_0'], axis=1)\n",
    "train = reduce_mem_usage(train)\n",
    "test  = reduce_mem_usage(test)\n",
    "\n",
    "X = train.copy()\n",
    "X_test = test.copy()\n",
    "\n",
    "assert len(X.columns) == len(X_test.columns), f'X と X_test のサイズが違います X: {len(X.columns)}, X_test: {len(X_test.columns)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, test, full_train, full_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(256)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    #x = Dropout(0.4)(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    out1 = Dense(2, activation=\"linear\")(x)#mulliken charge 2\n",
    "    out2 = Dense(6, activation=\"linear\")(x)#tensor 6(xx,yy,zz)\n",
    "    out3 = Dense(12, activation=\"linear\")(x)#tensor 12(others) \n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = Dense(1, activation=\"linear\")(x)#scalar_coupling_constant    \n",
    "    model = Model(inputs=inp, outputs=[out,out1,out2,out3])\n",
    "#     model = Model(inputs=inp, outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Function\n",
    "I rely a lot on loss plots to detect when learning has stopped as well as when overfitting begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, label):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Loss for %s' % label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    _= plt.legend(['Train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Routine\n",
    "\n",
    "A bunch of stuff happens here.  Pay attention to the callbacks.  I train a different model for each molecule type, which allows for future retraining.  If you have kept your network the same (except for dropout, etc.), and want to retrain for a few more epochs without having to go back to the beginning, then set the retrain flag to False and it will grab the trained models as starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 127), (10000, 127))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0 out of [0 3 1 4 2 6 5 7] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:617: DataConversionWarning:\n",
      "\n",
      "Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:462: DataConversionWarning:\n",
      "\n",
      "Data with input dtype int8, float64 were all converted to float64 by StandardScaler.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (32,) but got array with shape (127,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (32,) but got array with shape (127,)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mol_types=X[\"type\"].unique()\n",
    "# test_prediction = pd.DataFrame({'ind': list(X_test.index), 'type': X_test['type'].values, 'prediction': [0] * len(X_test)})\n",
    "test_prediction = np.zeros(len(X_test))\n",
    "cv_score=[]\n",
    "cv_score_total=0\n",
    "epoch_n = 300\n",
    "verbose = 0\n",
    "batch_size = 2048\n",
    "    \n",
    "# Set to True if we want to X from scratch.  False will reuse saved models as a starting point.\n",
    "retrain =False\n",
    "mol_type_int = [0, 3, 1, 4, 2, 6, 5, 7]\n",
    "mol_type_str = ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN']\n",
    "\n",
    "# Set up GPU preferences\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 2} ) \n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "sess = tf.Session(config=config) \n",
    "K.set_session(sess)\n",
    "\n",
    "start_time=datetime.now()\n",
    "\n",
    "# Loop through each molecule type\n",
    "for i, mol_type in enumerate(mol_types):\n",
    "    model_name_rd = ('../model/molecule_model_%s.hdf5' % mol_type_str[0])\n",
    "    model_name_wrt = (f'../model/nb{nb}_molecule_model_{mol_type}.hdf5')\n",
    "    print('Training %s' % mol_type, 'out of', mol_types, '\\n')\n",
    "    \n",
    "    X_=X[X[\"type\"]==mol_type]\n",
    "    X_test_=X_test[X_test[\"type\"]==mol_type]\n",
    "    \n",
    "#     # Here's our best features.  We think.\n",
    "#     input_features=[\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\",\"c_x\",\"c_y\",\"c_z\",\n",
    "#                     'x_closest_0','y_closest_0','z_closest_0','x_closest_1','y_closest_1','z_closest_1',\n",
    "#                     \"distance\",\"distance_center0\",\"distance_center1\", \"distance_c0\",\"distance_c1\",\"distance_f0\",\"distance_f1\",\n",
    "#                     \"cos_c0_c1\",\"cos_f0_f1\",\"cos_center0_center1\",\"cos_c0\",\"cos_c1\",\"cos_f0\",\"cos_f1\",\"cos_center0\",\"cos_center1\",\n",
    "#                     \"atom_n\"\n",
    "#                    ]\n",
    "    \n",
    "    # Standard Scaler from sklearn does seem to work better here than other Scalers\n",
    "#     input_data=StandardScaler().fit_transform(pd.concat([X_.loc[:,input_features],X_test_.loc[:,input_features]]))\n",
    "    input_data=StandardScaler().fit_transform(pd.concat([X_, X_test_]))\n",
    "    \n",
    "#     target_data=X_.loc[:,\"scalar_coupling_constant\"].values\n",
    "#     target_data_1=X_.loc[:,[\"charge_0\",\"charge_1\"]]\n",
    "#     target_data_2=X_.loc[:,[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]]\n",
    "#     target_data_3=X_.loc[:,[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]]\n",
    "    \n",
    "    #following parameters should be adjusted to control the loss function\n",
    "    #if all parameters are zero, attractors do not work. (-> simple neural network)\n",
    "    m1=1\n",
    "    m2=4\n",
    "    m3=1\n",
    "    y1 =m1*(StandardScaler().fit_transform(y1))\n",
    "    y2 =m2*(StandardScaler().fit_transform(y2))\n",
    "    y3 =m3*(StandardScaler().fit_transform(y3))\n",
    "    \n",
    "    # Simple split to provide us a validation set to do our CV checks with\n",
    "    train_index, cv_index = train_test_split(np.arange(len(X_)),random_state=111, test_size=0.1)\n",
    "    \n",
    "    # Split all our input and targets by train and cv indexes\n",
    "    train_input=input_data[train_index]\n",
    "    cv_input=input_data[cv_index]\n",
    "    \n",
    "    train_target=y[train_index]\n",
    "    cv_target=y[cv_index]\n",
    "    \n",
    "    train_target_1=y1[train_index]\n",
    "    cv_target_1=y1[cv_index]\n",
    "    \n",
    "    train_target_2=y2[train_index]\n",
    "    cv_target_2=y2[cv_index]\n",
    "    \n",
    "    train_target_3=y3[train_index]\n",
    "    cv_target_3=y3[cv_index]\n",
    "    \n",
    "    test_input=input_data[len(X_):,:]\n",
    "\n",
    "    # Build the Neural Net\n",
    "    nn_model=create_nn_model(train_input.shape[1])\n",
    "    \n",
    "    # If retrain==False, then we load a previous saved model as a starting point.\n",
    "    if not retrain:\n",
    "        nn_model = load_model(model_name_rd)\n",
    "        \n",
    "    nn_model.compile(loss='mae', optimizer=Adam())#, metrics=[auc])\n",
    "    \n",
    "    # Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=8,verbose=1, mode='auto', restore_best_weights=True)\n",
    "    # Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=7, min_lr=1e-6, mode='auto', verbose=1)\n",
    "    # Save the best value of the model for future use\n",
    "    sv_mod = callbacks.ModelCheckpoint(model_name_wrt, monitor='val_loss', save_best_only=True, period=1)\n",
    "\n",
    "    history = nn_model.fit(train_input,[train_target,train_target_1,train_target_2,train_target_3], \n",
    "            validation_data=(cv_input,[cv_target,cv_target_1,cv_target_2,cv_target_3]), \n",
    "            callbacks=[es, rlr, sv_mod], epochs=epoch_n, batch_size=batch_size, verbose=verbose)\n",
    "#             callbacks=[es, rlr], epochs=epoch_n, batch_size=batch_size, verbose=verbose)\n",
    "#     history = nn_model.fit(train_input,train_target, \n",
    "#             validation_data=(cv_input, cv_target), \n",
    "# #             callbacks=[es, rlr, sv_mod], epochs=epoch_n, batch_size=batch_size, verbose=verbose)\n",
    "#             callbacks=[es, rlr], epochs=epoch_n, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    cv_predict=nn_model.predict(cv_input)\n",
    "    plot_history(history, mol_type)\n",
    "    \n",
    "    accuracy=np.mean(np.abs(cv_target-cv_predict[0][:,0]))\n",
    "#     accuracy=np.mean(np.abs(cv_target-cv_predict[:,0]))\n",
    "    cv_score.append(np.log(accuracy))\n",
    "    cv_score_total+=np.log(accuracy)\n",
    "    \n",
    "    # Predict on the test data set using our trained model\n",
    "    test_predict=nn_model.predict(test_input)\n",
    "    \n",
    "#     from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    # for each molecule type we'll grab the predicted values\n",
    "    test_prediction[X_test[\"type\"]==mol_type]=test_predict[0][:,0]\n",
    "#     test_prediction[X_test[\"type\"]==mol_type]=test_predict[:,0]\n",
    "    K.clear_session()\n",
    "\n",
    "cv_score_total/=len(mol_types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n",
      "Total training time:  0:00:07.239610\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a40760b0e208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmol_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmol_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\": cv score is \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total cv score is\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv_score_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def submit(predictions):\n",
    "    if isSmallSet:\n",
    "        submit = pd.read_csv('../input/champs-scalar-coupling/sample_submission.csv')[:length]\n",
    "    else:\n",
    "        submit = pd.read_csv('../input/champs-scalar-coupling/sample_submission.csv')\n",
    "    print(len(submit), len(predictions))   \n",
    "    submit[\"scalar_coupling_constant\"] = predictions\n",
    "    path_submission = '../output/nb{}_submission_keras_{:.3f}.csv'.format(nb, cv_score_total)\n",
    "    submit.to_csv(path_submission, index=False)\n",
    "submit(test_prediction)\n",
    "\n",
    "print ('Total training time: ', datetime.now() - start_time)\n",
    "\n",
    "i=0\n",
    "for mol_type in mol_types: \n",
    "    print(mol_type,\": cv score is \",cv_score[i])\n",
    "    i+=1\n",
    "print(\"total cv score is\",cv_score_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
