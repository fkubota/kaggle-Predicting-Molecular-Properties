{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- nb29では、typeごとにパラメータを決めなかった。\n",
    "- このノートブックでは、パラメータをtypeごとに決める\n",
    "- nb29 で精度の良い`oof_fc`を計算するためである"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import everything I need :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "import glob\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from functools import partial\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "# from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 30\n",
    "isSmallSet = False\n",
    "length = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/champs-scalar-coupling/scalar_coupling_contributions.csv',\n",
       " '../input/champs-scalar-coupling/magnetic_shielding_tensors.csv',\n",
       " '../input/champs-scalar-coupling/structures.csv',\n",
       " '../input/champs-scalar-coupling/test.csv',\n",
       " '../input/champs-scalar-coupling/dipole_moments.csv',\n",
       " '../input/champs-scalar-coupling/potential_energy.csv',\n",
       " '../input/champs-scalar-coupling/sample_submission.csv',\n",
       " '../input/champs-scalar-coupling/train.csv',\n",
       " '../input/champs-scalar-coupling/mulliken_charges.csv']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../input/champs-scalar-coupling/'\n",
    "glob.glob(file_path + '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "path = file_path + 'train.csv'\n",
    "if isSmallSet:\n",
    "    train = pd.read_csv(path) [:length]\n",
    "else:\n",
    "    train = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "path = file_path + 'test.csv'\n",
    "if isSmallSet:\n",
    "    test = pd.read_csv(path)[:length]\n",
    "else:\n",
    "    test = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure\n",
    "path = file_path + 'structures.csv'\n",
    "structures = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4658147 rows in train data.\n",
      "There are 2505542 rows in test data.\n",
      "There are 85003 distinct molecules in train data.\n",
      "There are 45772 distinct molecules in test data.\n",
      "There are 29 unique atoms.\n",
      "There are 8 unique types.\n"
     ]
    }
   ],
   "source": [
    "if isSmallSet:\n",
    "    print('using SmallSet !!')\n",
    "    print('-------------------')\n",
    "\n",
    "print(f'There are {train.shape[0]} rows in train data.')\n",
    "print(f'There are {test.shape[0]} rows in test data.')\n",
    "\n",
    "print(f\"There are {train['molecule_name'].nunique()} distinct molecules in train data.\")\n",
    "print(f\"There are {test['molecule_name'].nunique()} distinct molecules in test data.\")\n",
    "print(f\"There are {train['atom_index_0'].nunique()} unique atoms.\")\n",
    "print(f\"There are {train['type'].nunique()} unique types.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## myFunc\n",
    "**metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_metric(df, preds):\n",
    "    df[\"prediction\"] = preds\n",
    "    maes = []\n",
    "    for t in df.type.unique():\n",
    "        y_true = df[df.type==t].scalar_coupling_constant.values\n",
    "        y_pred = df[df.type==t].prediction.values\n",
    "        mae = np.log(mean_absolute_error(y_true, y_pred))\n",
    "        maes.append(mae)\n",
    "    return np.mean(maes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**momory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info(df_1,df_2, atom_idx):\n",
    "    df = pd.merge(df_1, df_2, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "for atom_idx in [0,1]:\n",
    "    train = map_atom_info(train, structures, atom_idx)\n",
    "    test  = map_atom_info(test, structures, atom_idx)\n",
    "    \n",
    "    train = train.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                                        'x': f'x_{atom_idx}',\n",
    "                                        'y': f'y_{atom_idx}',\n",
    "                                        'z': f'z_{atom_idx}'})\n",
    "    test  =  test.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                                        'x': f'x_{atom_idx}',\n",
    "                                        'y': f'y_{atom_idx}',\n",
    "                                        'z': f'z_{atom_idx}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`type` の特徴量から、数字を抽出  \n",
    "例) 2JHC ---> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['type_0'] = train['type'].apply(lambda x: x[0])\n",
    "test['type_0'] = test['type'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add angle features\n",
    "- angle featue を追加するためには、dist が必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 630.81 Mb (11.2% reduction)\n",
      "Mem. usage decreased to 320.19 Mb (11.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "# dist\n",
    "train_p_0 = train[['x_0', 'y_0', 'z_0']].values\n",
    "train_p_1 = train[['x_1', 'y_1', 'z_1']].values\n",
    "test_p_0 = test[['x_0', 'y_0', 'z_0']].values\n",
    "test_p_1 = test[['x_1', 'y_1', 'z_1']].values\n",
    "\n",
    "train['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\n",
    "test['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\n",
    "train['dist_x'] = (train['x_0'] - train['x_1']) ** 2\n",
    "test['dist_x'] = (test['x_0'] - test['x_1']) ** 2\n",
    "train['dist_y'] = (train['y_0'] - train['y_1']) ** 2\n",
    "test['dist_y'] = (test['y_0'] - test['y_1']) ** 2\n",
    "train['dist_z'] = (train['z_0'] - train['z_1']) ** 2\n",
    "test['dist_z'] = (test['z_0'] - test['z_1']) ** 2\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1101.70 Mb (0.0% reduction)\n",
      "Mem. usage decreased to 573.47 Mb (0.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "def add_cos_features(df):\n",
    "    #I apologize for my poor coding skill. Please make the better one.\n",
    "#     print(train.shape)\n",
    "    df_temp=df.loc[:,[\"molecule_name\",\"atom_index_0\",\"atom_index_1\",\"dist\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n",
    "\n",
    "    df_temp_=df_temp.copy()\n",
    "    df_temp_= df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
    "                                       'atom_index_1': 'atom_index_0',\n",
    "                                       'x_0': 'x_1',\n",
    "                                       'y_0': 'y_1',\n",
    "                                       'z_0': 'z_1',\n",
    "                                       'x_1': 'x_0',\n",
    "                                       'y_1': 'y_0',\n",
    "                                       'z_1': 'z_0'})\n",
    "    df_temp=pd.concat((df_temp,df_temp_),axis=0)\n",
    "    df_temp[\"min_dist\"]=df_temp.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df_temp= df_temp[df_temp[\"min_dist\"]==df_temp[\"dist\"]]\n",
    "    df_temp=df_temp.drop(['x_0','y_0','z_0','min_dist'], axis=1)\n",
    "    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                     'atom_index_1': 'atom_index_closest',\n",
    "                                     'dist': 'dist_closest',\n",
    "                                     'x_1': 'x_closest',\n",
    "                                     'y_1': 'y_closest',\n",
    "                                     'z_1': 'z_closest'})\n",
    "\n",
    "    #delete duplicated rows (some atom pairs have perfectly same distance)\n",
    "    #This code is added based on Adriano Avelar's comment.\n",
    "    df_temp=df_temp.drop_duplicates(subset=['molecule_name', 'atom_index'])\n",
    "\n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df,df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
    "                                            'dist_closest': f'dist_closest_{atom_idx}',\n",
    "                                            'x_closest': f'x_closest_{atom_idx}',\n",
    "                                            'y_closest': f'y_closest_{atom_idx}',\n",
    "                                            'z_closest': f'z_closest_{atom_idx}'})\n",
    "\n",
    "    df[\"dist_0\"]=((df['x_0']-df['x_closest_0'])**2+(df['y_0']-df['y_closest_0'])**2+(df['z_0']-df['z_closest_0'])**2)**(1/2)\n",
    "    df[\"dist_1\"]=((df['x_1']-df['x_closest_1'])**2+(df['y_1']-df['y_closest_1'])**2+(df['z_1']-df['z_closest_1'])**2)**(1/2)\n",
    "    df[\"vec_0_x\"]=(df['x_0']-df['x_closest_0'])/df[\"dist_0\"]\n",
    "    df[\"vec_0_y\"]=(df['y_0']-df['y_closest_0'])/df[\"dist_0\"]\n",
    "    df[\"vec_0_z\"]=(df['z_0']-df['z_closest_0'])/df[\"dist_0\"]\n",
    "    df[\"vec_1_x\"]=(df['x_1']-df['x_closest_1'])/df[\"dist_1\"]\n",
    "    df[\"vec_1_y\"]=(df['y_1']-df['y_closest_1'])/df[\"dist_1\"]\n",
    "    df[\"vec_1_z\"]=(df['z_1']-df['z_closest_1'])/df[\"dist_1\"]\n",
    "    df[\"vec_x\"]=(df['x_1']-df['x_0'])/df[\"dist\"]\n",
    "    df[\"vec_y\"]=(df['y_1']-df['y_0'])/df[\"dist\"]\n",
    "    df[\"vec_z\"]=(df['z_1']-df['z_0'])/df[\"dist\"]\n",
    "    df[\"cos_0_1\"]=df[\"vec_0_x\"]*df[\"vec_1_x\"]+df[\"vec_0_y\"]*df[\"vec_1_y\"]+df[\"vec_0_z\"]*df[\"vec_1_z\"]\n",
    "    df[\"cos_0\"]=df[\"vec_0_x\"]*df[\"vec_x\"]+df[\"vec_0_y\"]*df[\"vec_y\"]+df[\"vec_0_z\"]*df[\"vec_z\"]\n",
    "    df[\"cos_1\"]=df[\"vec_1_x\"]*df[\"vec_x\"]+df[\"vec_1_y\"]*df[\"vec_y\"]+df[\"vec_1_z\"]*df[\"vec_z\"]\n",
    "    df=df.drop(['vec_0_x','vec_0_y','vec_0_z','vec_1_x','vec_1_y','vec_1_z','vec_x','vec_y','vec_z'], axis=1)\n",
    "    \n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train = add_cos_features(train)\n",
    "test  = add_cos_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、**分子単位**で統計量を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df['molecule_couples'] = df.groupby('molecule_name')['id'].transform('count')\n",
    "    df['molecule_dist_mean'] = df.groupby('molecule_name')['dist'].transform('mean')\n",
    "    df['molecule_dist_min'] = df.groupby('molecule_name')['dist'].transform('min')\n",
    "    df['molecule_dist_max'] = df.groupby('molecule_name')['dist'].transform('max')\n",
    "    df['atom_0_couples_count'] = df.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\n",
    "    df['atom_1_couples_count'] = df.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n",
    "    \n",
    "    df[f'molecule_atom_index_0_x_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['x_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_y_1_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_y_1_mean_diff'] = df[f'molecule_atom_index_0_y_1_mean'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_mean_div'] = df[f'molecule_atom_index_0_y_1_mean'] / df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_max'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('max')\n",
    "    df[f'molecule_atom_index_0_y_1_max_diff'] = df[f'molecule_atom_index_0_y_1_max'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_z_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['z_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_dist_mean_diff'] = df[f'molecule_atom_index_0_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_mean_div'] = df[f'molecule_atom_index_0_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_0_dist_max_diff'] = df[f'molecule_atom_index_0_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max_div'] = df[f'molecule_atom_index_0_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_0_dist_min_diff'] = df[f'molecule_atom_index_0_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min_div'] = df[f'molecule_atom_index_0_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_std_diff'] = df[f'molecule_atom_index_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std_div'] = df[f'molecule_atom_index_0_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_1_dist_mean_diff'] = df[f'molecule_atom_index_1_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean_div'] = df[f'molecule_atom_index_1_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_1_dist_max_diff'] = df[f'molecule_atom_index_1_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max_div'] = df[f'molecule_atom_index_1_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_1_dist_min_diff'] = df[f'molecule_atom_index_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min_div'] = df[f'molecule_atom_index_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_1_dist_std_diff'] = df[f'molecule_atom_index_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std_div'] = df[f'molecule_atom_index_1_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_mean'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_1_dist_min'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_1_dist_min_diff'] = df[f'molecule_atom_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_1_dist_min_div'] = df[f'molecule_atom_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_std'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_1_dist_std_diff'] = df[f'molecule_atom_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_0_dist_std'] = df.groupby(['molecule_name', 'type_0'])['dist'].transform('std')\n",
    "    df[f'molecule_type_0_dist_std_diff'] = df[f'molecule_type_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean'] = df.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n",
    "    df[f'molecule_type_dist_mean_diff'] = df[f'molecule_type_dist_mean'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean_div'] = df[f'molecule_type_dist_mean'] / df['dist']\n",
    "    df[f'molecule_type_dist_max'] = df.groupby(['molecule_name', 'type'])['dist'].transform('max')\n",
    "    df[f'molecule_type_dist_min'] = df.groupby(['molecule_name', 'type'])['dist'].transform('min')\n",
    "    df[f'molecule_type_dist_std'] = df.groupby(['molecule_name', 'type'])['dist'].transform('std')\n",
    "    df[f'molecule_type_dist_std_diff'] = df[f'molecule_type_dist_std'] - df['dist']\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2896.42 Mb (3.0% reduction)\n",
      "CPU times: user 13min 9s, sys: 39.8 s, total: 13min 49s\n",
      "Wall time: 13min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = create_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1538.82 Mb (3.0% reduction)\n",
      "CPU times: user 6min 58s, sys: 19.4 s, total: 7min 18s\n",
      "Wall time: 7min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = create_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "LabelEncode\n",
    "- `atom_1` = {H, C, N}\n",
    "- `type_0` = {1, 2, 3}\n",
    "- `type`   = {2JHC, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['atom_1', 'type_0', 'type']:\n",
    "    if f in train.columns:\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[f].values) + list(test[f].values))\n",
    "        train[f] = lbl.transform(list(train[f].values))\n",
    "        test[f] = lbl.transform(list(test[f].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**show features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "      <th>atom_0</th>\n",
       "      <th>x_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>z_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>z_1</th>\n",
       "      <th>type_0</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_x</th>\n",
       "      <th>dist_y</th>\n",
       "      <th>dist_z</th>\n",
       "      <th>atom_index_closest_0</th>\n",
       "      <th>dist_closest_0</th>\n",
       "      <th>x_closest_0</th>\n",
       "      <th>y_closest_0</th>\n",
       "      <th>z_closest_0</th>\n",
       "      <th>atom_index_closest_1</th>\n",
       "      <th>dist_closest_1</th>\n",
       "      <th>x_closest_1</th>\n",
       "      <th>y_closest_1</th>\n",
       "      <th>z_closest_1</th>\n",
       "      <th>dist_0</th>\n",
       "      <th>dist_1</th>\n",
       "      <th>cos_0_1</th>\n",
       "      <th>cos_0</th>\n",
       "      <th>cos_1</th>\n",
       "      <th>molecule_couples</th>\n",
       "      <th>molecule_dist_mean</th>\n",
       "      <th>molecule_dist_min</th>\n",
       "      <th>molecule_dist_max</th>\n",
       "      <th>atom_0_couples_count</th>\n",
       "      <th>atom_1_couples_count</th>\n",
       "      <th>molecule_atom_index_0_x_1_std</th>\n",
       "      <th>molecule_atom_index_0_y_1_mean</th>\n",
       "      <th>molecule_atom_index_0_y_1_mean_diff</th>\n",
       "      <th>molecule_atom_index_0_y_1_mean_div</th>\n",
       "      <th>molecule_atom_index_0_y_1_max</th>\n",
       "      <th>molecule_atom_index_0_y_1_max_diff</th>\n",
       "      <th>molecule_atom_index_0_y_1_std</th>\n",
       "      <th>molecule_atom_index_0_z_1_std</th>\n",
       "      <th>molecule_atom_index_0_dist_mean</th>\n",
       "      <th>molecule_atom_index_0_dist_mean_diff</th>\n",
       "      <th>molecule_atom_index_0_dist_mean_div</th>\n",
       "      <th>molecule_atom_index_0_dist_max</th>\n",
       "      <th>molecule_atom_index_0_dist_max_diff</th>\n",
       "      <th>molecule_atom_index_0_dist_max_div</th>\n",
       "      <th>molecule_atom_index_0_dist_min</th>\n",
       "      <th>molecule_atom_index_0_dist_min_diff</th>\n",
       "      <th>molecule_atom_index_0_dist_min_div</th>\n",
       "      <th>molecule_atom_index_0_dist_std</th>\n",
       "      <th>molecule_atom_index_0_dist_std_diff</th>\n",
       "      <th>molecule_atom_index_0_dist_std_div</th>\n",
       "      <th>molecule_atom_index_1_dist_mean</th>\n",
       "      <th>molecule_atom_index_1_dist_mean_diff</th>\n",
       "      <th>molecule_atom_index_1_dist_mean_div</th>\n",
       "      <th>molecule_atom_index_1_dist_max</th>\n",
       "      <th>molecule_atom_index_1_dist_max_diff</th>\n",
       "      <th>molecule_atom_index_1_dist_max_div</th>\n",
       "      <th>molecule_atom_index_1_dist_min</th>\n",
       "      <th>molecule_atom_index_1_dist_min_diff</th>\n",
       "      <th>molecule_atom_index_1_dist_min_div</th>\n",
       "      <th>molecule_atom_index_1_dist_std</th>\n",
       "      <th>molecule_atom_index_1_dist_std_diff</th>\n",
       "      <th>molecule_atom_index_1_dist_std_div</th>\n",
       "      <th>molecule_atom_1_dist_mean</th>\n",
       "      <th>molecule_atom_1_dist_min</th>\n",
       "      <th>molecule_atom_1_dist_min_diff</th>\n",
       "      <th>molecule_atom_1_dist_min_div</th>\n",
       "      <th>molecule_atom_1_dist_std</th>\n",
       "      <th>molecule_atom_1_dist_std_diff</th>\n",
       "      <th>molecule_type_0_dist_std</th>\n",
       "      <th>molecule_type_0_dist_std_diff</th>\n",
       "      <th>molecule_type_dist_mean</th>\n",
       "      <th>molecule_type_dist_mean_diff</th>\n",
       "      <th>molecule_type_dist_mean_div</th>\n",
       "      <th>molecule_type_dist_max</th>\n",
       "      <th>molecule_type_dist_min</th>\n",
       "      <th>molecule_type_dist_std</th>\n",
       "      <th>molecule_type_dist_std_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.8076</td>\n",
       "      <td>H</td>\n",
       "      <td>0.00215</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>0</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>1.192105</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>3</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>0.333335</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.333335</td>\n",
       "      <td>10</td>\n",
       "      <td>1.506668</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>1.783158</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.727907</td>\n",
       "      <td>1.358754</td>\n",
       "      <td>0.272949</td>\n",
       "      <td>1.251380</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.377947</td>\n",
       "      <td>0.182278</td>\n",
       "      <td>0.727957</td>\n",
       "      <td>1.610344</td>\n",
       "      <td>0.518391</td>\n",
       "      <td>1.474738</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>0.691204</td>\n",
       "      <td>1.632998</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.345594</td>\n",
       "      <td>-0.746359</td>\n",
       "      <td>0.316492</td>\n",
       "      <td>1.09195</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-1.09195</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.091950</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-1.091950</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-1.091950</td>\n",
       "      <td>1.091950</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-1.091950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-11.2570</td>\n",
       "      <td>H</td>\n",
       "      <td>0.00215</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>1</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>1.019253</td>\n",
       "      <td>2.160261</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>0</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>1.091952</td>\n",
       "      <td>-0.333287</td>\n",
       "      <td>-0.816483</td>\n",
       "      <td>0.816482</td>\n",
       "      <td>10</td>\n",
       "      <td>1.506668</td>\n",
       "      <td>1.091946</td>\n",
       "      <td>1.783158</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.727907</td>\n",
       "      <td>1.358754</td>\n",
       "      <td>-0.104998</td>\n",
       "      <td>0.928268</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182278</td>\n",
       "      <td>0.727957</td>\n",
       "      <td>1.610344</td>\n",
       "      <td>-0.172776</td>\n",
       "      <td>0.903105</td>\n",
       "      <td>1.783157</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>1.000021</td>\n",
       "      <td>1.091953</td>\n",
       "      <td>-0.691167</td>\n",
       "      <td>0.612383</td>\n",
       "      <td>0.345594</td>\n",
       "      <td>-1.437526</td>\n",
       "      <td>0.193814</td>\n",
       "      <td>1.78312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.783146</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-1.783106</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-1.783106</td>\n",
       "      <td>1.783146</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.000015</td>\n",
       "      <td>1.783158</td>\n",
       "      <td>1.783120</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-1.783106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     molecule_name  atom_index_0  atom_index_1  type  \\\n",
       "0   0  dsgdb9nsd_000001             1             0     0   \n",
       "1   1  dsgdb9nsd_000001             1             2     3   \n",
       "\n",
       "   scalar_coupling_constant atom_0      x_0       y_0       z_0  atom_1  \\\n",
       "0                   84.8076      H  0.00215 -0.006031  0.001976       0   \n",
       "1                  -11.2570      H  0.00215 -0.006031  0.001976       1   \n",
       "\n",
       "        x_1       y_1       z_1  type_0      dist    dist_x    dist_y  \\\n",
       "0 -0.012698  1.085804  0.008001       0  1.091953  0.000220  1.192105   \n",
       "1  1.011731  1.463751  0.000277       1  1.783120  1.019253  2.160261   \n",
       "\n",
       "     dist_z  atom_index_closest_0  dist_closest_0  x_closest_0  y_closest_0  \\\n",
       "0  0.000036                     0        1.091953    -0.012698     1.085804   \n",
       "1  0.000003                     0        1.091953    -0.012698     1.085804   \n",
       "\n",
       "   z_closest_0  atom_index_closest_1  dist_closest_1  x_closest_1  \\\n",
       "0     0.008001                     3        1.091946    -0.540815   \n",
       "1     0.008001                     0        1.091952    -0.012698   \n",
       "\n",
       "   y_closest_1  z_closest_1    dist_0    dist_1   cos_0_1     cos_0     cos_1  \\\n",
       "0     1.447527    -0.876644  1.091953  1.091946  0.333335 -1.000000 -0.333335   \n",
       "1     1.085804     0.008001  1.091953  1.091952 -0.333287 -0.816483  0.816482   \n",
       "\n",
       "   molecule_couples  molecule_dist_mean  molecule_dist_min  molecule_dist_max  \\\n",
       "0                10            1.506668           1.091946           1.783158   \n",
       "1                10            1.506668           1.091946           1.783158   \n",
       "\n",
       "   atom_0_couples_count  atom_1_couples_count  molecule_atom_index_0_x_1_std  \\\n",
       "0                     4                     4                       0.727907   \n",
       "1                     4                     1                       0.727907   \n",
       "\n",
       "   molecule_atom_index_0_y_1_mean  molecule_atom_index_0_y_1_mean_diff  \\\n",
       "0                        1.358754                             0.272949   \n",
       "1                        1.358754                            -0.104998   \n",
       "\n",
       "   molecule_atom_index_0_y_1_mean_div  molecule_atom_index_0_y_1_max  \\\n",
       "0                            1.251380                       1.463751   \n",
       "1                            0.928268                       1.463751   \n",
       "\n",
       "   molecule_atom_index_0_y_1_max_diff  molecule_atom_index_0_y_1_std  \\\n",
       "0                            0.377947                       0.182278   \n",
       "1                            0.000000                       0.182278   \n",
       "\n",
       "   molecule_atom_index_0_z_1_std  molecule_atom_index_0_dist_mean  \\\n",
       "0                       0.727957                         1.610344   \n",
       "1                       0.727957                         1.610344   \n",
       "\n",
       "   molecule_atom_index_0_dist_mean_diff  molecule_atom_index_0_dist_mean_div  \\\n",
       "0                              0.518391                             1.474738   \n",
       "1                             -0.172776                             0.903105   \n",
       "\n",
       "   molecule_atom_index_0_dist_max  molecule_atom_index_0_dist_max_diff  \\\n",
       "0                        1.783157                             0.691204   \n",
       "1                        1.783157                             0.000037   \n",
       "\n",
       "   molecule_atom_index_0_dist_max_div  molecule_atom_index_0_dist_min  \\\n",
       "0                            1.632998                        1.091953   \n",
       "1                            1.000021                        1.091953   \n",
       "\n",
       "   molecule_atom_index_0_dist_min_diff  molecule_atom_index_0_dist_min_div  \\\n",
       "0                             0.000000                            1.000000   \n",
       "1                            -0.691167                            0.612383   \n",
       "\n",
       "   molecule_atom_index_0_dist_std  molecule_atom_index_0_dist_std_diff  \\\n",
       "0                        0.345594                            -0.746359   \n",
       "1                        0.345594                            -1.437526   \n",
       "\n",
       "   molecule_atom_index_0_dist_std_div  molecule_atom_index_1_dist_mean  \\\n",
       "0                            0.316492                          1.09195   \n",
       "1                            0.193814                          1.78312   \n",
       "\n",
       "   molecule_atom_index_1_dist_mean_diff  molecule_atom_index_1_dist_mean_div  \\\n",
       "0                             -0.000003                             0.999997   \n",
       "1                              0.000000                             1.000000   \n",
       "\n",
       "   molecule_atom_index_1_dist_max  molecule_atom_index_1_dist_max_diff  \\\n",
       "0                        1.091953                                  0.0   \n",
       "1                        1.783120                                  0.0   \n",
       "\n",
       "   molecule_atom_index_1_dist_max_div  molecule_atom_index_1_dist_min  \\\n",
       "0                                 1.0                        1.091946   \n",
       "1                                 1.0                        1.783120   \n",
       "\n",
       "   molecule_atom_index_1_dist_min_diff  molecule_atom_index_1_dist_min_div  \\\n",
       "0                            -0.000007                            0.999994   \n",
       "1                             0.000000                            1.000000   \n",
       "\n",
       "   molecule_atom_index_1_dist_std  molecule_atom_index_1_dist_std_diff  \\\n",
       "0                        0.000003                             -1.09195   \n",
       "1                             NaN                                  NaN   \n",
       "\n",
       "   molecule_atom_index_1_dist_std_div  molecule_atom_1_dist_mean  \\\n",
       "0                            0.000003                   1.091950   \n",
       "1                                 NaN                   1.783146   \n",
       "\n",
       "   molecule_atom_1_dist_min  molecule_atom_1_dist_min_diff  \\\n",
       "0                  1.091946                      -0.000007   \n",
       "1                  1.783120                       0.000000   \n",
       "\n",
       "   molecule_atom_1_dist_min_div  molecule_atom_1_dist_std  \\\n",
       "0                      0.999994                  0.000003   \n",
       "1                      1.000000                  0.000014   \n",
       "\n",
       "   molecule_atom_1_dist_std_diff  molecule_type_0_dist_std  \\\n",
       "0                      -1.091950                  0.000003   \n",
       "1                      -1.783106                  0.000014   \n",
       "\n",
       "   molecule_type_0_dist_std_diff  molecule_type_dist_mean  \\\n",
       "0                      -1.091950                 1.091950   \n",
       "1                      -1.783106                 1.783146   \n",
       "\n",
       "   molecule_type_dist_mean_diff  molecule_type_dist_mean_div  \\\n",
       "0                     -0.000003                     0.999997   \n",
       "1                      0.000027                     1.000015   \n",
       "\n",
       "   molecule_type_dist_max  molecule_type_dist_min  molecule_type_dist_std  \\\n",
       "0                1.091953                1.091946                0.000003   \n",
       "1                1.783158                1.783120                0.000014   \n",
       "\n",
       "   molecule_type_dist_std_diff  \n",
       "0                    -1.091950  \n",
       "1                    -1.783106  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'molecule_name', 'atom_index_0', 'atom_index_1', 'type',\n",
      "       'scalar_coupling_constant', 'atom_0', 'x_0', 'y_0', 'z_0', 'atom_1',\n",
      "       'x_1', 'y_1', 'z_1', 'type_0', 'dist', 'dist_x', 'dist_y', 'dist_z',\n",
      "       'atom_index_closest_0', 'dist_closest_0', 'x_closest_0', 'y_closest_0',\n",
      "       'z_closest_0', 'atom_index_closest_1', 'dist_closest_1', 'x_closest_1',\n",
      "       'y_closest_1', 'z_closest_1', 'dist_0', 'dist_1', 'cos_0_1', 'cos_0',\n",
      "       'cos_1', 'molecule_couples', 'molecule_dist_mean', 'molecule_dist_min',\n",
      "       'molecule_dist_max', 'atom_0_couples_count', 'atom_1_couples_count',\n",
      "       'molecule_atom_index_0_x_1_std', 'molecule_atom_index_0_y_1_mean',\n",
      "       'molecule_atom_index_0_y_1_mean_diff',\n",
      "       'molecule_atom_index_0_y_1_mean_div', 'molecule_atom_index_0_y_1_max',\n",
      "       'molecule_atom_index_0_y_1_max_diff', 'molecule_atom_index_0_y_1_std',\n",
      "       'molecule_atom_index_0_z_1_std', 'molecule_atom_index_0_dist_mean',\n",
      "       'molecule_atom_index_0_dist_mean_diff',\n",
      "       'molecule_atom_index_0_dist_mean_div', 'molecule_atom_index_0_dist_max',\n",
      "       'molecule_atom_index_0_dist_max_diff',\n",
      "       'molecule_atom_index_0_dist_max_div', 'molecule_atom_index_0_dist_min',\n",
      "       'molecule_atom_index_0_dist_min_diff',\n",
      "       'molecule_atom_index_0_dist_min_div', 'molecule_atom_index_0_dist_std',\n",
      "       'molecule_atom_index_0_dist_std_diff',\n",
      "       'molecule_atom_index_0_dist_std_div', 'molecule_atom_index_1_dist_mean',\n",
      "       'molecule_atom_index_1_dist_mean_diff',\n",
      "       'molecule_atom_index_1_dist_mean_div', 'molecule_atom_index_1_dist_max',\n",
      "       'molecule_atom_index_1_dist_max_diff',\n",
      "       'molecule_atom_index_1_dist_max_div', 'molecule_atom_index_1_dist_min',\n",
      "       'molecule_atom_index_1_dist_min_diff',\n",
      "       'molecule_atom_index_1_dist_min_div', 'molecule_atom_index_1_dist_std',\n",
      "       'molecule_atom_index_1_dist_std_diff',\n",
      "       'molecule_atom_index_1_dist_std_div', 'molecule_atom_1_dist_mean',\n",
      "       'molecule_atom_1_dist_min', 'molecule_atom_1_dist_min_diff',\n",
      "       'molecule_atom_1_dist_min_div', 'molecule_atom_1_dist_std',\n",
      "       'molecule_atom_1_dist_std_diff', 'molecule_type_0_dist_std',\n",
      "       'molecule_type_0_dist_std_diff', 'molecule_type_dist_mean',\n",
      "       'molecule_type_dist_mean_diff', 'molecule_type_dist_mean_div',\n",
      "       'molecule_type_dist_max', 'molecule_type_dist_min',\n",
      "       'molecule_type_dist_std', 'molecule_type_dist_std_diff'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = reduce_mem_usage(train)\n",
    "# test  = reduce_mem_usage(test)\n",
    "y = train['scalar_coupling_constant']\n",
    "train = train.drop(['id', 'molecule_name', 'atom_0', 'scalar_coupling_constant'], axis=1)\n",
    "test  =  test.drop(['id', 'molecule_name', 'atom_0'], axis=1)\n",
    "\n",
    "X = train.copy()\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2608"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X,\n",
    "                                                  y,\n",
    "                                                  test_size = 0.30, \n",
    "                                                  random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = multiprocessing.cpu_count() -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define searched space\n",
    "hyper_space = {'objective': 'regression',\n",
    "               'metric':'mae',\n",
    "               'boosting':'gbdt',\n",
    "               'max_depth':  hp.choice('max_depth', [5, 8, 10, 12, 15]),\n",
    "#                'num_leaves': hp.choice('num_leaves', [100, 250, 500, 650, 750, 1000,1300]),\n",
    "               'num_leaves': hp.choice('num_leaves', [10, 50, 100, 250]),\n",
    "               'subsample': hp.choice('subsample', [.3, .5, .7, .8, 1]),\n",
    "               'subsample_freq': 1,\n",
    "               'colsample_bytree': hp.choice('colsample_bytree', [ .6, .7, .8, .9, 1]),\n",
    "               'learning_rate': hp.choice('learning_rate', [.1, .2, .3]),\n",
    "               'reg_alpha': hp.choice('reg_alpha', [.1, .2, .3, .4, .5, .6]),\n",
    "               'reg_lambda':  hp.choice('reg_lambda', [.1, .2, .3, .4, .5, .6]),               \n",
    "               'min_child_samples': hp.choice('min_child_samples', [20, 45, 70, 100]),\n",
    "               'verbosity': -1,\n",
    "               'bagging_seed': 11,\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "- Training of type 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.78913\tvalid_1's l1: 2.05966\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.78913\tvalid_1's l1: 2.05966\n",
      "cv_score:0.7225389899051997\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.78913\tvalid_1's l1: 2.05966\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.78913\tvalid_1's l1: 2.05966\n",
      "cv_score:0.7225389899051997\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.27756\tvalid_1's l1: 1.94423\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.27756\tvalid_1's l1: 1.94423\n",
      "cv_score:0.6648638078294542\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.14546\tvalid_1's l1: 1.80978\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.14546\tvalid_1's l1: 1.80978\n",
      "cv_score:0.5932046669139552\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.57739\tvalid_1's l1: 1.93985\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.57739\tvalid_1's l1: 1.93985\n",
      "cv_score:0.6626093005126563\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.68405\tvalid_1's l1: 1.9739\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.68405\tvalid_1's l1: 1.9739\n",
      "cv_score:0.6800116414440335\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 2.4856\tvalid_1's l1: 2.52841\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 2.4856\tvalid_1's l1: 2.52841\n",
      "cv_score:0.9275911072924077\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 2.19863\tvalid_1's l1: 2.3017\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 2.19863\tvalid_1's l1: 2.3017\n",
      "cv_score:0.8336466052836989\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 2.49208\tvalid_1's l1: 2.53767\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 2.49208\tvalid_1's l1: 2.53767\n",
      "cv_score:0.9312455351261928\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.3711\tvalid_1's l1: 1.78497\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.3711\tvalid_1's l1: 1.78497\n",
      "cv_score:0.5794037399720633\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.78941\tvalid_1's l1: 1.99602\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.78941\tvalid_1's l1: 1.99602\n",
      "cv_score:0.6911529516357771\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.37036\tvalid_1's l1: 1.78102\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.37036\tvalid_1's l1: 1.78102\n",
      "cv_score:0.5771851704349826\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 2.41566\tvalid_1's l1: 2.47079\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 2.41566\tvalid_1's l1: 2.47079\n",
      "cv_score:0.9045387764739764\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.81137\tvalid_1's l1: 2.01265\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.81137\tvalid_1's l1: 2.01265\n",
      "cv_score:0.699454295804161\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 2.71723\tvalid_1's l1: 2.7436\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 2.71723\tvalid_1's l1: 2.7436\n",
      "cv_score:1.0092703703481016\n",
      "100%|██████████| 15/15 [08:53<00:00, 29.77s/it, best loss: 0.5771851704349826]\n",
      "BEST PARAMETERS: {'bagging_seed': 11, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 15, 'metric': 'mae', 'min_child_samples': 70, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.3, 'subsample': 0.8, 'subsample_freq': 1, 'verbosity': -1}\n",
      "--------------------------------------------------------------------------------\n",
      "- Training of type 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.04819\tvalid_1's l1: 1.15322\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.04819\tvalid_1's l1: 1.15322\n",
      "cv_score:0.1425549530690909\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.04819\tvalid_1's l1: 1.15322\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.04819\tvalid_1's l1: 1.15322\n",
      "cv_score:0.1425549530690909\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.316133\tvalid_1's l1: 0.820022\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.316133\tvalid_1's l1: 0.820022\n",
      "cv_score:-0.1984241955904282\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.164031\tvalid_1's l1: 0.845993\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.164031\tvalid_1's l1: 0.845993\n",
      "cv_score:-0.1672443458993754\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.0283745\tvalid_1's l1: 0.949151\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.0283745\tvalid_1's l1: 0.949151\n",
      "cv_score:-0.05218741358471905\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.274181\tvalid_1's l1: 0.815538\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.274181\tvalid_1's l1: 0.815538\n",
      "cv_score:-0.2039073299547317\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.567287\tvalid_1's l1: 0.899205\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.567287\tvalid_1's l1: 0.899205\n",
      "cv_score:-0.10624420049407791\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.786048\tvalid_1's l1: 1.1181\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.786048\tvalid_1's l1: 1.1181\n",
      "cv_score:0.11162764907641169\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.237153\tvalid_1's l1: 0.997615\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.237153\tvalid_1's l1: 0.997615\n",
      "cv_score:-0.002387630860503816\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.274181\tvalid_1's l1: 0.815538\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.274181\tvalid_1's l1: 0.815538\n",
      "cv_score:-0.2039073299547317\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.469139\tvalid_1's l1: 0.857914\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.469139\tvalid_1's l1: 0.857914\n",
      "cv_score:-0.1532514621963588\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[73]\ttraining's l1: 0.847239\tvalid_1's l1: 1.19302\n",
      "cv_score:0.17648577963670803\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.699046\tvalid_1's l1: 1.00726\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.699046\tvalid_1's l1: 1.00726\n",
      "cv_score:0.007236868614599227\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.567884\tvalid_1's l1: 0.90112\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.567884\tvalid_1's l1: 0.90112\n",
      "cv_score:-0.10411700991212015\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.847745\tvalid_1's l1: 1.04631\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.847745\tvalid_1's l1: 1.04631\n",
      "cv_score:0.04527014667622942\n",
      "100%|██████████| 15/15 [02:33<00:00,  5.70s/it, best loss: -0.2039073299547317]\n",
      "BEST PARAMETERS: {'bagging_seed': 11, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 10, 'metric': 'mae', 'min_child_samples': 20, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.6, 'subsample': 0.7, 'subsample_freq': 1, 'verbosity': -1}\n",
      "--------------------------------------------------------------------------------\n",
      "- Training of type 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.75634\tvalid_1's l1: 0.84501\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.75634\tvalid_1's l1: 0.84501\n",
      "cv_score:-0.16840714169611867\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.75634\tvalid_1's l1: 0.84501\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.75634\tvalid_1's l1: 0.84501\n",
      "cv_score:-0.16840714169611642\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.09763\tvalid_1's l1: 1.11512\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[499]\ttraining's l1: 1.09763\tvalid_1's l1: 1.11508\n",
      "cv_score:0.10892280737539249\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.748181\tvalid_1's l1: 0.810011\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.748181\tvalid_1's l1: 0.810011\n",
      "cv_score:-0.2107071661999617\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.594534\tvalid_1's l1: 0.711453\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.594534\tvalid_1's l1: 0.711453\n",
      "cv_score:-0.3404460414098898\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.456258\tvalid_1's l1: 0.722036\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.456258\tvalid_1's l1: 0.722036\n",
      "cv_score:-0.32567994236554804\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.868441\tvalid_1's l1: 0.904137\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.868441\tvalid_1's l1: 0.904137\n",
      "cv_score:-0.10077430890617224\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.592696\tvalid_1's l1: 0.708884\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.592696\tvalid_1's l1: 0.708884\n",
      "cv_score:-0.3440630189489616\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.592696\tvalid_1's l1: 0.708884\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.592696\tvalid_1's l1: 0.708884\n",
      "cv_score:-0.34406269552674207\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.513718\tvalid_1's l1: 0.778317\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.513718\tvalid_1's l1: 0.778317\n",
      "cv_score:-0.2506219379560142\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.86126\tvalid_1's l1: 0.896763\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.86126\tvalid_1's l1: 0.896763\n",
      "cv_score:-0.10896394909890743\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.02007\tvalid_1's l1: 1.03996\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.02007\tvalid_1's l1: 1.03996\n",
      "cv_score:0.039181812819860704\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.592696\tvalid_1's l1: 0.708884\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.592696\tvalid_1's l1: 0.708884\n",
      "cv_score:-0.34406269552674207\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.02955\tvalid_1's l1: 1.0507\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.02955\tvalid_1's l1: 1.0507\n",
      "cv_score:0.04945613291446892\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 1.17825\tvalid_1's l1: 1.18875\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 1.17825\tvalid_1's l1: 1.18875\n",
      "cv_score:0.17289856390135583\n",
      "100%|██████████| 15/15 [15:50<00:00, 48.21s/it, best loss: -0.3440630189489616]\n",
      "BEST PARAMETERS: {'bagging_seed': 11, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 15, 'metric': 'mae', 'min_child_samples': 100, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.3, 'subsample': 0.5, 'subsample_freq': 1, 'verbosity': -1}\n",
      "--------------------------------------------------------------------------------\n",
      "- Training of type 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.467185\tvalid_1's l1: 0.490803\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.467185\tvalid_1's l1: 0.490803\n",
      "cv_score:-0.7117123797784187\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.467185\tvalid_1's l1: 0.490803\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.467185\tvalid_1's l1: 0.490803\n",
      "cv_score:-0.7117123797785172\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.391956\tvalid_1's l1: 0.435311\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.391956\tvalid_1's l1: 0.435311\n",
      "cv_score:-0.831693953038576\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.236338\tvalid_1's l1: 0.406507\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.236338\tvalid_1's l1: 0.406507\n",
      "cv_score:-0.9001552881007229\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.186628\tvalid_1's l1: 0.428214\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.186628\tvalid_1's l1: 0.428214\n",
      "cv_score:-0.8481324927780932\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.219753\tvalid_1's l1: 0.427697\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.219753\tvalid_1's l1: 0.427697\n",
      "cv_score:-0.8493400063192863\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.2589\tvalid_1's l1: 0.402291\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.2589\tvalid_1's l1: 0.402291\n",
      "cv_score:-0.9105807588383503\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.239203\tvalid_1's l1: 0.399008\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.239203\tvalid_1's l1: 0.399008\n",
      "cv_score:-0.918773967970061\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.476294\tvalid_1's l1: 0.499638\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.476294\tvalid_1's l1: 0.499638\n",
      "cv_score:-0.6938714604812992\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.261306\tvalid_1's l1: 0.379848\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.261306\tvalid_1's l1: 0.379848\n",
      "cv_score:-0.9679852240817117\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.319394\tvalid_1's l1: 0.439056\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.319394\tvalid_1's l1: 0.439056\n",
      "cv_score:-0.8231274466016101\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.503344\tvalid_1's l1: 0.519761\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.503344\tvalid_1's l1: 0.519761\n",
      "cv_score:-0.6543856095540337\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.287718\tvalid_1's l1: 0.403707\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.287718\tvalid_1's l1: 0.403707\n",
      "cv_score:-0.9070652192798704\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.2615\tvalid_1's l1: 0.378879\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.2615\tvalid_1's l1: 0.378879\n",
      "cv_score:-0.9705380309821503\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.509421\tvalid_1's l1: 0.524688\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.509421\tvalid_1's l1: 0.524688\n",
      "cv_score:-0.6449515289630479\n",
      "100%|██████████| 15/15 [06:04<00:00, 18.53s/it, best loss: -0.9705380309821503]\n",
      "BEST PARAMETERS: {'bagging_seed': 11, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 12, 'metric': 'mae', 'min_child_samples': 70, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.2, 'subsample': 0.8, 'subsample_freq': 1, 'verbosity': -1}\n",
      "--------------------------------------------------------------------------------\n",
      "- Training of type 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.205364\tvalid_1's l1: 0.351507\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.205364\tvalid_1's l1: 0.351507\n",
      "cv_score:-1.0455267743815568\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.205364\tvalid_1's l1: 0.351507\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.205364\tvalid_1's l1: 0.351507\n",
      "cv_score:-1.0455267743819068\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.199354\tvalid_1's l1: 0.336514\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.199354\tvalid_1's l1: 0.336514\n",
      "cv_score:-1.0891145434603726\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.130234\tvalid_1's l1: 0.314368\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.130234\tvalid_1's l1: 0.314368\n",
      "cv_score:-1.1571917662992284\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.522974\tvalid_1's l1: 0.550937\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.522974\tvalid_1's l1: 0.550937\n",
      "cv_score:-0.5961349922588478\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.170948\tvalid_1's l1: 0.390345\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.170948\tvalid_1's l1: 0.390345\n",
      "cv_score:-0.9407245193609027\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.251091\tvalid_1's l1: 0.378813\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.251091\tvalid_1's l1: 0.378813\n",
      "cv_score:-0.9707114690769053\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.375523\tvalid_1's l1: 0.474673\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.375523\tvalid_1's l1: 0.474673\n",
      "cv_score:-0.7451297629747409\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.521415\tvalid_1's l1: 0.54898\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.521415\tvalid_1's l1: 0.54898\n",
      "cv_score:-0.5996939952883324\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.166706\tvalid_1's l1: 0.33501\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.166706\tvalid_1's l1: 0.33501\n",
      "cv_score:-1.0935935895037725\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.307985\tvalid_1's l1: 0.40055\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.307985\tvalid_1's l1: 0.40055\n",
      "cv_score:-0.9149154871854388\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.339471\tvalid_1's l1: 0.444949\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.339471\tvalid_1's l1: 0.444949\n",
      "cv_score:-0.8097958166621789\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.444281\tvalid_1's l1: 0.494029\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.444281\tvalid_1's l1: 0.494029\n",
      "cv_score:-0.7051608796542506\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.263455\tvalid_1's l1: 0.36291\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.263455\tvalid_1's l1: 0.36291\n",
      "cv_score:-1.0135993738983033\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.209842\tvalid_1's l1: 0.422886\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.209842\tvalid_1's l1: 0.422886\n",
      "cv_score:-0.860653677825994\n",
      "100%|██████████| 15/15 [04:06<00:00, 12.08s/it, best loss: -1.1571917662992284]\n",
      "BEST PARAMETERS: {'bagging_seed': 11, 'boosting': 'gbdt', 'colsample_bytree': 1, 'learning_rate': 0.1, 'max_depth': 15, 'metric': 'mae', 'min_child_samples': 100, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.4, 'subsample': 0.7, 'subsample_freq': 1, 'verbosity': -1}\n",
      "--------------------------------------------------------------------------------\n",
      "- Training of type 5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.579627\tvalid_1's l1: 0.671086\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.579627\tvalid_1's l1: 0.671086\n",
      "cv_score:-0.39885854241397584\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.579126\tvalid_1's l1: 0.671342\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.579126\tvalid_1's l1: 0.671342\n",
      "cv_score:-0.398476593110858\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.765563\tvalid_1's l1: 0.786577\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.765563\tvalid_1's l1: 0.786577\n",
      "cv_score:-0.24006457260938535\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.58657\tvalid_1's l1: 0.674564\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.58657\tvalid_1's l1: 0.674564\n",
      "cv_score:-0.3936890188673884\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.568002\tvalid_1's l1: 0.645781\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.568002\tvalid_1's l1: 0.645781\n",
      "cv_score:-0.43729548503262744\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.568002\tvalid_1's l1: 0.645781\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.568002\tvalid_1's l1: 0.645781\n",
      "cv_score:-0.43729543347099487\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.927598\tvalid_1's l1: 0.935628\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.927598\tvalid_1's l1: 0.935628\n",
      "cv_score:-0.06653721065038044\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.835136\tvalid_1's l1: 0.853969\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.835136\tvalid_1's l1: 0.853969\n",
      "cv_score:-0.1578605522409024\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.636896\tvalid_1's l1: 0.68391\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.636896\tvalid_1's l1: 0.68391\n",
      "cv_score:-0.3799288513098092\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.457102\tvalid_1's l1: 0.58435\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.457102\tvalid_1's l1: 0.58435\n",
      "cv_score:-0.5372544909091379\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.930751\tvalid_1's l1: 0.939223\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.930751\tvalid_1's l1: 0.939223\n",
      "cv_score:-0.06270268234404293\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.682103\tvalid_1's l1: 0.720641\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.682103\tvalid_1's l1: 0.720641\n",
      "cv_score:-0.32761427298990825\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.53446\tvalid_1's l1: 0.609184\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.53446\tvalid_1's l1: 0.609184\n",
      "cv_score:-0.49563569028265964\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.953419\tvalid_1's l1: 0.961072\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.953419\tvalid_1's l1: 0.961072\n",
      "cv_score:-0.03970646133904638\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.698799\tvalid_1's l1: 0.737353\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.698799\tvalid_1's l1: 0.737353\n",
      "cv_score:-0.3046888168560945\n",
      "100%|██████████| 15/15 [23:04<00:00, 89.67s/it, best loss: -0.5372544909091379] \n",
      "BEST PARAMETERS: {'bagging_seed': 11, 'boosting': 'gbdt', 'colsample_bytree': 1, 'learning_rate': 0.2, 'max_depth': 15, 'metric': 'mae', 'min_child_samples': 100, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.6, 'subsample': 1, 'subsample_freq': 1, 'verbosity': -1}\n",
      "--------------------------------------------------------------------------------\n",
      "- Training of type 6\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.176368\tvalid_1's l1: 0.30142\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.176368\tvalid_1's l1: 0.30142\n",
      "cv_score:-1.1992496203447587\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.176368\tvalid_1's l1: 0.301421\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.176368\tvalid_1's l1: 0.301421\n",
      "cv_score:-1.199248940460668\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.520505\tvalid_1's l1: 0.525088\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.520505\tvalid_1's l1: 0.525088\n",
      "cv_score:-0.6441899109577415\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.398517\tvalid_1's l1: 0.426353\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.398517\tvalid_1's l1: 0.426353\n",
      "cv_score:-0.8524882862359371\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.325568\tvalid_1's l1: 0.367007\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.325568\tvalid_1's l1: 0.367007\n",
      "cv_score:-1.0023746468511199\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.258722\tvalid_1's l1: 0.331817\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.258722\tvalid_1's l1: 0.331817\n",
      "cv_score:-1.103171182685272\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.182752\tvalid_1's l1: 0.30951\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.182752\tvalid_1's l1: 0.30951\n",
      "cv_score:-1.1727653988346538\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.170942\tvalid_1's l1: 0.317186\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.170942\tvalid_1's l1: 0.317186\n",
      "cv_score:-1.1482681160132056\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.5144\tvalid_1's l1: 0.520924\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.5144\tvalid_1's l1: 0.520924\n",
      "cv_score:-0.6521503071166551\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.277955\tvalid_1's l1: 0.343997\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.277955\tvalid_1's l1: 0.343997\n",
      "cv_score:-1.0671234152117328\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.317417\tvalid_1's l1: 0.374419\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.317417\tvalid_1's l1: 0.374419\n",
      "cv_score:-0.9823811007088648\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.394859\tvalid_1's l1: 0.416548\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.394859\tvalid_1's l1: 0.416548\n",
      "cv_score:-0.8757540001838521\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.243829\tvalid_1's l1: 0.325246\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.243829\tvalid_1's l1: 0.325246\n",
      "cv_score:-1.123172700845742\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.320243\tvalid_1's l1: 0.364395\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.320243\tvalid_1's l1: 0.364395\n",
      "cv_score:-1.0095157270650308\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.30204\tvalid_1's l1: 0.342833\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.30204\tvalid_1's l1: 0.342833\n",
      "cv_score:-1.0705115934537202\n",
      "100%|██████████| 15/15 [08:31<00:00, 26.23s/it, best loss: -1.1992496203447587]\n",
      "BEST PARAMETERS: {'bagging_seed': 11, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'learning_rate': 0.2, 'max_depth': 15, 'metric': 'mae', 'min_child_samples': 70, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.2, 'subsample': 1, 'subsample_freq': 1, 'verbosity': -1}\n",
      "--------------------------------------------------------------------------------\n",
      "- Training of type 7\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.184057\tvalid_1's l1: 0.231382\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.184057\tvalid_1's l1: 0.231382\n",
      "cv_score:-1.4636866243762465\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.184057\tvalid_1's l1: 0.231382\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.184057\tvalid_1's l1: 0.231382\n",
      "cv_score:-1.463686624376249\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.195436\tvalid_1's l1: 0.239608\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.195436\tvalid_1's l1: 0.239608\n",
      "cv_score:-1.428750064423346\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.287245\tvalid_1's l1: 0.316379\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.287245\tvalid_1's l1: 0.316379\n",
      "cv_score:-1.1508142861835062\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.11109\tvalid_1's l1: 0.218586\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.11109\tvalid_1's l1: 0.218586\n",
      "cv_score:-1.5205766672832859\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.11109\tvalid_1's l1: 0.218586\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.11109\tvalid_1's l1: 0.218586\n",
      "cv_score:-1.5205766672833385\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.0476274\tvalid_1's l1: 0.202213\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.0476274\tvalid_1's l1: 0.202213\n",
      "cv_score:-1.5984355054232124\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.10654\tvalid_1's l1: 0.213635\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.10654\tvalid_1's l1: 0.213635\n",
      "cv_score:-1.5434861447141712\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.0625259\tvalid_1's l1: 0.205173\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.0625259\tvalid_1's l1: 0.205173\n",
      "cv_score:-1.5839010933499034\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.0325986\tvalid_1's l1: 0.221135\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.0325986\tvalid_1's l1: 0.221135\n",
      "cv_score:-1.508982165881458\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.225997\tvalid_1's l1: 0.267563\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.225997\tvalid_1's l1: 0.267563\n",
      "cv_score:-1.318400536234709\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.278995\tvalid_1's l1: 0.2983\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.278995\tvalid_1's l1: 0.2983\n",
      "cv_score:-1.2096545104755416\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[84]\ttraining's l1: 0.208006\tvalid_1's l1: 0.279513\n",
      "cv_score:-1.2747058805980684\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.11591\tvalid_1's l1: 0.216723\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.11591\tvalid_1's l1: 0.216723\n",
      "cv_score:-1.5291341598404822\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[500]\ttraining's l1: 0.312352\tvalid_1's l1: 0.324286\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\ttraining's l1: 0.312352\tvalid_1's l1: 0.324286\n",
      "cv_score:-1.126130382875673\n",
      "100%|██████████| 15/15 [05:13<00:00, 17.72s/it, best loss: -1.5984355054232124]\n",
      "BEST PARAMETERS: {'bagging_seed': 11, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'learning_rate': 0.2, 'max_depth': 12, 'metric': 'mae', 'min_child_samples': 45, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.4, 'subsample': 0.8, 'subsample_freq': 1, 'verbosity': -1}\n"
     ]
    }
   ],
   "source": [
    "# type ごとの学習 \n",
    "\n",
    "# feature_importance  = pd.DataFrame()\n",
    "# X_short      = pd.DataFrame({'ind': list(X.index),      'type': X['type'].values,      'oof': [0] * len(X), 'target': y_fc})\n",
    "# X_short_test = pd.DataFrame({'ind': list(X_test.index), 'type': X_test['type'].values, 'prediction': [0] * len(X_test)})\n",
    "best_params_list = []\n",
    "for t in sorted(X_train['type'].unique()):\n",
    "    print('-'*80)\n",
    "    print(f'- Training of type {t}')\n",
    "    X_t_train = X_train.loc[X_train['type'] == t]\n",
    "    X_t_valid = X_valid.loc[X_valid['type'] == t]\n",
    "    y_t_train = y_train[X_train['type'] == t]\n",
    "    y_t_valid = y_valid[X_valid['type'] == t]\n",
    "    \n",
    "    \n",
    "    # evaluate_metric\n",
    "    def evaluate_metric(params):\n",
    "    #     model_lgb = lgb.train(params, lgtrain, 500, \n",
    "    #                           valid_sets=[lgtrain, lgval], early_stopping_rounds=20, \n",
    "    #                           verbose_eval=500)\n",
    "    #     model_lgb = lgb.train(para)\n",
    "        model_lgb = lgb.LGBMRegressor(**params, n_jobs=N_JOBS, n_estimators=500) \n",
    "        model_lgb.fit(X_t_train, y_t_train,\n",
    "                  eval_set=[(X_t_train, y_t_train), (X_t_valid, y_t_valid)],\n",
    "                  verbose=500,\n",
    "                  early_stopping_rounds=100)\n",
    "\n",
    "        pred = model_lgb.predict(X_t_valid)\n",
    "\n",
    "        _X_t_valid = X_t_valid.copy()\n",
    "        _X_t_valid['scalar_coupling_constant'] = y_t_valid\n",
    "        cv_score = kaggle_metric(_X_t_valid, pred)\n",
    "        _X_t_valid = _X_t_valid.drop(['scalar_coupling_constant'], axis=1)\n",
    "\n",
    "#         print(f'mae(valid): {mean_absolute_error(y_t_valid, pred)}')\n",
    "        print(f'cv_score:{cv_score}')\n",
    "\n",
    "        return {\n",
    "            'loss': cv_score,\n",
    "            'status': STATUS_OK,\n",
    "            'stats_running': STATUS_RUNNING\n",
    "        }\n",
    "    \n",
    "    \n",
    "    # hyperopt\n",
    "    # Trail\n",
    "    trials = Trials()\n",
    "\n",
    "    # Set algoritm parameters\n",
    "    algo = partial(tpe.suggest, \n",
    "                   n_startup_jobs=-1)\n",
    "\n",
    "    # Seting the number of Evals\n",
    "    MAX_EVALS= 15\n",
    "\n",
    "    # Fit Tree Parzen Estimator\n",
    "    best_vals = fmin(evaluate_metric, space=hyper_space, verbose=1,\n",
    "                     algo=algo, max_evals=MAX_EVALS, trials=trials)\n",
    "\n",
    "    # Print best parameters\n",
    "    best_params = space_eval(hyper_space, best_vals)\n",
    "    best_params_list.append(best_params)\n",
    "    print(\"BEST PARAMETERS: \" + str(best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bagging_seed': 11,\n",
       "  'boosting': 'gbdt',\n",
       "  'colsample_bytree': 0.7,\n",
       "  'learning_rate': 0.1,\n",
       "  'max_depth': 15,\n",
       "  'metric': 'mae',\n",
       "  'min_child_samples': 70,\n",
       "  'num_leaves': 250,\n",
       "  'objective': 'regression',\n",
       "  'reg_alpha': 0.6,\n",
       "  'reg_lambda': 0.3,\n",
       "  'subsample': 0.8,\n",
       "  'subsample_freq': 1,\n",
       "  'verbosity': -1},\n",
       " {'bagging_seed': 11,\n",
       "  'boosting': 'gbdt',\n",
       "  'colsample_bytree': 0.7,\n",
       "  'learning_rate': 0.1,\n",
       "  'max_depth': 10,\n",
       "  'metric': 'mae',\n",
       "  'min_child_samples': 20,\n",
       "  'num_leaves': 100,\n",
       "  'objective': 'regression',\n",
       "  'reg_alpha': 0.5,\n",
       "  'reg_lambda': 0.6,\n",
       "  'subsample': 0.7,\n",
       "  'subsample_freq': 1,\n",
       "  'verbosity': -1},\n",
       " {'bagging_seed': 11,\n",
       "  'boosting': 'gbdt',\n",
       "  'colsample_bytree': 0.9,\n",
       "  'learning_rate': 0.1,\n",
       "  'max_depth': 15,\n",
       "  'metric': 'mae',\n",
       "  'min_child_samples': 100,\n",
       "  'num_leaves': 250,\n",
       "  'objective': 'regression',\n",
       "  'reg_alpha': 0.2,\n",
       "  'reg_lambda': 0.3,\n",
       "  'subsample': 0.5,\n",
       "  'subsample_freq': 1,\n",
       "  'verbosity': -1},\n",
       " {'bagging_seed': 11,\n",
       "  'boosting': 'gbdt',\n",
       "  'colsample_bytree': 0.8,\n",
       "  'learning_rate': 0.2,\n",
       "  'max_depth': 12,\n",
       "  'metric': 'mae',\n",
       "  'min_child_samples': 70,\n",
       "  'num_leaves': 100,\n",
       "  'objective': 'regression',\n",
       "  'reg_alpha': 0.6,\n",
       "  'reg_lambda': 0.2,\n",
       "  'subsample': 0.8,\n",
       "  'subsample_freq': 1,\n",
       "  'verbosity': -1},\n",
       " {'bagging_seed': 11,\n",
       "  'boosting': 'gbdt',\n",
       "  'colsample_bytree': 1,\n",
       "  'learning_rate': 0.1,\n",
       "  'max_depth': 15,\n",
       "  'metric': 'mae',\n",
       "  'min_child_samples': 100,\n",
       "  'num_leaves': 250,\n",
       "  'objective': 'regression',\n",
       "  'reg_alpha': 0.6,\n",
       "  'reg_lambda': 0.4,\n",
       "  'subsample': 0.7,\n",
       "  'subsample_freq': 1,\n",
       "  'verbosity': -1},\n",
       " {'bagging_seed': 11,\n",
       "  'boosting': 'gbdt',\n",
       "  'colsample_bytree': 1,\n",
       "  'learning_rate': 0.2,\n",
       "  'max_depth': 15,\n",
       "  'metric': 'mae',\n",
       "  'min_child_samples': 100,\n",
       "  'num_leaves': 250,\n",
       "  'objective': 'regression',\n",
       "  'reg_alpha': 0.2,\n",
       "  'reg_lambda': 0.6,\n",
       "  'subsample': 1,\n",
       "  'subsample_freq': 1,\n",
       "  'verbosity': -1},\n",
       " {'bagging_seed': 11,\n",
       "  'boosting': 'gbdt',\n",
       "  'colsample_bytree': 0.7,\n",
       "  'learning_rate': 0.2,\n",
       "  'max_depth': 15,\n",
       "  'metric': 'mae',\n",
       "  'min_child_samples': 70,\n",
       "  'num_leaves': 250,\n",
       "  'objective': 'regression',\n",
       "  'reg_alpha': 0.6,\n",
       "  'reg_lambda': 0.2,\n",
       "  'subsample': 1,\n",
       "  'subsample_freq': 1,\n",
       "  'verbosity': -1},\n",
       " {'bagging_seed': 11,\n",
       "  'boosting': 'gbdt',\n",
       "  'colsample_bytree': 0.7,\n",
       "  'learning_rate': 0.2,\n",
       "  'max_depth': 12,\n",
       "  'metric': 'mae',\n",
       "  'min_child_samples': 45,\n",
       "  'num_leaves': 250,\n",
       "  'objective': 'regression',\n",
       "  'reg_alpha': 0.1,\n",
       "  'reg_lambda': 0.4,\n",
       "  'subsample': 0.8,\n",
       "  'subsample_freq': 1,\n",
       "  'verbosity': -1}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_list = [{'bagging_seed': 11,\n",
    "  'boosting': 'gbdt',\n",
    "  'colsample_bytree': 0.7,\n",
    "  'learning_rate': 0.1,\n",
    "  'max_depth': 15,\n",
    "  'metric': 'mae',\n",
    "  'min_child_samples': 70,\n",
    "  'num_leaves': 250,\n",
    "  'objective': 'regression',\n",
    "  'reg_alpha': 0.6,\n",
    "  'reg_lambda': 0.3,\n",
    "  'subsample': 0.8,\n",
    "  'subsample_freq': 1,\n",
    "  'verbosity': -1},\n",
    " {'bagging_seed': 11,\n",
    "  'boosting': 'gbdt',\n",
    "  'colsample_bytree': 0.7,\n",
    "  'learning_rate': 0.1,\n",
    "  'max_depth': 10,\n",
    "  'metric': 'mae',\n",
    "  'min_child_samples': 20,\n",
    "  'num_leaves': 100,\n",
    "  'objective': 'regression',\n",
    "  'reg_alpha': 0.5,\n",
    "  'reg_lambda': 0.6,\n",
    "  'subsample': 0.7,\n",
    "  'subsample_freq': 1,\n",
    "  'verbosity': -1},\n",
    " {'bagging_seed': 11,\n",
    "  'boosting': 'gbdt',\n",
    "  'colsample_bytree': 0.9,\n",
    "  'learning_rate': 0.1,\n",
    "  'max_depth': 15,\n",
    "  'metric': 'mae',\n",
    "  'min_child_samples': 100,\n",
    "  'num_leaves': 250,\n",
    "  'objective': 'regression',\n",
    "  'reg_alpha': 0.2,\n",
    "  'reg_lambda': 0.3,\n",
    "  'subsample': 0.5,\n",
    "  'subsample_freq': 1,\n",
    "  'verbosity': -1},\n",
    " {'bagging_seed': 11,\n",
    "  'boosting': 'gbdt',\n",
    "  'colsample_bytree': 0.8,\n",
    "  'learning_rate': 0.2,\n",
    "  'max_depth': 12,\n",
    "  'metric': 'mae',\n",
    "  'min_child_samples': 70,\n",
    "  'num_leaves': 100,\n",
    "  'objective': 'regression',\n",
    "  'reg_alpha': 0.6,\n",
    "  'reg_lambda': 0.2,\n",
    "  'subsample': 0.8,\n",
    "  'subsample_freq': 1,\n",
    "  'verbosity': -1},\n",
    " {'bagging_seed': 11,\n",
    "  'boosting': 'gbdt',\n",
    "  'colsample_bytree': 1,\n",
    "  'learning_rate': 0.1,\n",
    "  'max_depth': 15,\n",
    "  'metric': 'mae',\n",
    "  'min_child_samples': 100,\n",
    "  'num_leaves': 250,\n",
    "  'objective': 'regression',\n",
    "  'reg_alpha': 0.6,\n",
    "  'reg_lambda': 0.4,\n",
    "  'subsample': 0.7,\n",
    "  'subsample_freq': 1,\n",
    "  'verbosity': -1},\n",
    " {'bagging_seed': 11,\n",
    "  'boosting': 'gbdt',\n",
    "  'colsample_bytree': 1,\n",
    "  'learning_rate': 0.2,\n",
    "  'max_depth': 15,\n",
    "  'metric': 'mae',\n",
    "  'min_child_samples': 100,\n",
    "  'num_leaves': 250,\n",
    "  'objective': 'regression',\n",
    "  'reg_alpha': 0.2,\n",
    "  'reg_lambda': 0.6,\n",
    "  'subsample': 1,\n",
    "  'subsample_freq': 1,\n",
    "  'verbosity': -1},\n",
    " {'bagging_seed': 11,\n",
    "  'boosting': 'gbdt',\n",
    "  'colsample_bytree': 0.7,\n",
    "  'learning_rate': 0.2,\n",
    "  'max_depth': 15,\n",
    "  'metric': 'mae',\n",
    "  'min_child_samples': 70,\n",
    "  'num_leaves': 250,\n",
    "  'objective': 'regression',\n",
    "  'reg_alpha': 0.6,\n",
    "  'reg_lambda': 0.2,\n",
    "  'subsample': 1,\n",
    "  'subsample_freq': 1,\n",
    "  'verbosity': -1},\n",
    " {'bagging_seed': 11,\n",
    "  'boosting': 'gbdt',\n",
    "  'colsample_bytree': 0.7,\n",
    "  'learning_rate': 0.2,\n",
    "  'max_depth': 12,\n",
    "  'metric': 'mae',\n",
    "  'min_child_samples': 45,\n",
    "  'num_leaves': 250,\n",
    "  'objective': 'regression',\n",
    "  'reg_alpha': 0.1,\n",
    "  'reg_lambda': 0.4,\n",
    "  'subsample': 0.8,\n",
    "  'subsample_freq': 1,\n",
    "  'verbosity': -1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lgb_params_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
