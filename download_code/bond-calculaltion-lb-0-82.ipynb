{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/EdsonAvelar/auc/master/molecular_banner.png\" width=1900px height=400px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Molecular Properties\n",
    "<h3 style=\"color:red\">If this kernel helps you, up vote to keep me motivated üòÅ<br>Thanks!</h3>\n",
    "\n",
    "\n",
    "<h3> Can you measure the magnetic interactions between a pair of atoms? </h3>\n",
    "\n",
    "This kernel is a combination of multiple kernels. The goal is to organize and explain the code to beginner competitors like me.<br>\n",
    "This Kernels creates lots of new features and uses lightgbm as model.\n",
    "> Update: Using Bond Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents:\n",
    "\n",
    "**1. [Problem Definition](#id1)** <br>\n",
    "**2. [Get the Data (Collect / Obtain)](#id2)** <br>\n",
    "**3. [Load the Dataset](#id3)** <br>\n",
    "**4. [Data Pre-processing](#id4)** <br>\n",
    "**5. [Model](#id5)** <br>\n",
    "**6. [Visualization and Analysis of Results](#id6)** <br>\n",
    "**7. [Submittion](#id7)** <br>\n",
    "**8. [References](#ref)** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id1\"></a> <br> \n",
    "# **1. Problem Definition:** \n",
    "\n",
    "This challenge aims to predict interactions between atoms. The main task is develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant)<br>\n",
    "\n",
    "In this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC), and any features you are able to create from the molecule structure (xyz) files.\n",
    "\n",
    "**Data**\n",
    "* **train.csv** - the training set, where the first column (molecule_name) is the name of the molecule where the coupling constant originates, the second (atom_index_0) and third column (atom_index_1) is the atom indices of the atom-pair creating the coupling and the fourth column (**scalar_coupling_constant**) is the scalar coupling constant that we want to be able to predict\n",
    "* **test.csv** - the test set; same info as train, without the target variable\n",
    "* **sample_submission.csv** - a sample submission file in the correct format\n",
    "* **structures.csv** - this file contains the same information as the individual xyz structure files, but in a single file\n",
    "\n",
    "**Additional Data**<br>\n",
    "*NOTE: additional data is provided for the molecules in Train only!*\n",
    "* **scalar_coupling_contributions.csv** - The scalar coupling constants in train.csv are a sum of four terms. The first column (**molecule_name**) are the name of the molecule, the second (**atom_index_0**) and third column (**atom_index_1**) are the atom indices of the atom-pair, the fourth column indicates the **type** of coupling, the fifth column (**fc**) is the Fermi Contact contribution, the sixth column (**sd**) is the Spin-dipolar contribution, the seventh column (**pso**) is the Paramagnetic spin-orbit contribution and the eighth column (**dso**) is the Diamagnetic spin-orbit contribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id2\"></a> <br> \n",
    "# **2. Get the Data (Collect / Obtain):** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All imports used in this kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('notebook')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "import altair as alt\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import gc\n",
    "from numba import jit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import altair as alt\n",
    "from altair.vega import v3\n",
    "from IPython.display import HTML\n",
    "alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All function used in this kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>    requirejs.config({\n",
       "        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
       "        paths: {'vega': 'https://cdn.jsdelivr.net/npm/vega@v3.3.1?noext', 'vega-lib': 'https://cdn.jsdelivr.net/npm/vega-lib?noext', 'vega-lite': 'https://cdn.jsdelivr.net/npm/vega-lite@v2.6.0?noext', 'vega-embed': 'https://cdn.jsdelivr.net/npm/vega-embed@3?noext'}\n",
       "    });\n",
       "    </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n",
    "def prepare_altair():\n",
    "    \"\"\"\n",
    "    Helper function to prepare altair for working.\n",
    "    \"\"\"\n",
    "    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v3.SCHEMA_VERSION\n",
    "    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n",
    "    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n",
    "    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n",
    "    noext = \"?noext\"\n",
    "    \n",
    "    paths = {\n",
    "        'vega': vega_url + noext,\n",
    "        'vega-lib': vega_lib_url + noext,\n",
    "        'vega-lite': vega_lite_url + noext,\n",
    "        'vega-embed': vega_embed_url + noext\n",
    "    }\n",
    "    \n",
    "    workaround = f\"\"\"    requirejs.config({{\n",
    "        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
    "        paths: {paths}\n",
    "    }});\n",
    "    \"\"\"\n",
    "    \n",
    "    return workaround\n",
    "    \n",
    "\n",
    "def add_autoincrement(render_func):\n",
    "    # Keep track of unique <div/> IDs\n",
    "    cache = {}\n",
    "    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n",
    "        if autoincrement:\n",
    "            if id in cache:\n",
    "                counter = 1 + cache[id]\n",
    "                cache[id] = counter\n",
    "            else:\n",
    "                cache[id] = 0\n",
    "            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n",
    "        else:\n",
    "            if id not in cache:\n",
    "                cache[id] = 0\n",
    "            actual_id = id\n",
    "        return render_func(chart, id=actual_id)\n",
    "    # Cache will stay outside and \n",
    "    return wrapped\n",
    "           \n",
    "\n",
    "@add_autoincrement\n",
    "def render(chart, id=\"vega-chart\"):\n",
    "    \"\"\"\n",
    "    Helper function to plot altair visualizations.\n",
    "    \"\"\"\n",
    "    chart_str = \"\"\"\n",
    "    <div id=\"{id}\"></div><script>\n",
    "    require([\"vega-embed\"], function(vg_embed) {{\n",
    "        const spec = {chart};     \n",
    "        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n",
    "        console.log(\"anything?\");\n",
    "    }});\n",
    "    console.log(\"really...anything?\");\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return HTML(\n",
    "        chart_str.format(\n",
    "            id=id,\n",
    "            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "    \n",
    "\n",
    "@jit\n",
    "def fast_auc(y_true, y_prob):\n",
    "    \"\"\"\n",
    "    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    nfalse = 0\n",
    "    auc = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n):\n",
    "        y_i = y_true[i]\n",
    "        nfalse += (1 - y_i)\n",
    "        auc += y_i * nfalse\n",
    "    auc /= (nfalse * (n - nfalse))\n",
    "    return auc\n",
    "\n",
    "\n",
    "def eval_auc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast auc eval function for lgb.\n",
    "    \"\"\"\n",
    "    return 'auc', fast_auc(y_true, y_pred), True\n",
    "\n",
    "\n",
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    \"\"\"\n",
    "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
    "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "    \"\"\"\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
    "    \n",
    "\n",
    "def train_model_regression(X, X_test, y, params, folds, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'sklearn_scoring_function': metrics.mean_absolute_error},\n",
    "                    'group_mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'scoring_function': group_mean_log_mae},\n",
    "                    'mse': {'lgb_metric_name': 'mse',\n",
    "                        'catboost_metric_name': 'MSE',\n",
    "                        'sklearn_scoring_function': metrics.mean_squared_error}\n",
    "                    }\n",
    "\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        if eval_metric != 'group_mae':\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "        else:\n",
    "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict\n",
    "    \n",
    "\n",
    "\n",
    "def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns == None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n",
    "                        'catboost_metric_name': 'AUC',\n",
    "                        'sklearn_scoring_function': metrics.roc_auc_score},\n",
    "                    }\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros((len(X), len(set(y.values))))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros((len(X_test), oof.shape[1]))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict_proba(X_valid)\n",
    "            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid\n",
    "        scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1]))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict\n",
    "\n",
    "# setting up altair\n",
    "workaround = prepare_altair()\n",
    "HTML(\"\".join((\n",
    "    \"<script>\",\n",
    "    workaround,\n",
    "    \"</script>\",\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id3\"></a> <br> \n",
    "# **3. Load the Dataset** \n",
    "\n",
    "Let's load all necessary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape is -> rows: 4658147 cols:6\n",
      "Test dataset shape is  -> rows: 2505542 cols:5\n",
      "Sub dataset shape is  -> rows: 2505542 cols:2\n",
      "Structures dataset shape is  -> rows: 2358657 cols:6\n",
      "Scalar_coupling_contributions dataset shape is  -> rows: 4658147 cols:8\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "structures = pd.read_csv('../input/structures.csv')\n",
    "scalar_coupling_contributions = pd.read_csv('../input/scalar_coupling_contributions.csv')\n",
    "\n",
    "print('Train dataset shape is -> rows: {} cols:{}'.format(train.shape[0],train.shape[1]))\n",
    "print('Test dataset shape is  -> rows: {} cols:{}'.format(test.shape[0],test.shape[1]))\n",
    "print('Sub dataset shape is  -> rows: {} cols:{}'.format(sub.shape[0],sub.shape[1]))\n",
    "print('Structures dataset shape is  -> rows: {} cols:{}'.format(structures.shape[0],structures.shape[1]))\n",
    "print('Scalar_coupling_contributions dataset shape is  -> rows: {} cols:{}'.format(scalar_coupling_contributions.shape[0],\n",
    "                                                                                   scalar_coupling_contributions.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an fast model/feature evaluation, get only 10% of dataset. Final submission must remove/coments this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_default = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsize = round(0.10*train.shape[0])\\ntrain = train[:size]\\ntest = test[:size]\\nsub = sub[:size]\\nstructures = structures[:size]\\nscalar_coupling_contributions = scalar_coupling_contributions[:size]\\n\\nprint('Train dataset shape is now rows: {} cols:{}'.format(train.shape[0],train.shape[1]))\\nprint('Test dataset shape is now rows: {} cols:{}'.format(test.shape[0],test.shape[1]))\\nprint('Sub dataset shape is now rows: {} cols:{}'.format(sub.shape[0],sub.shape[1]))\\nprint('Structures dataset shape is now rows: {} cols:{}'.format(structures.shape[0],structures.shape[1]))\\nprint('Scalar_coupling_contributions dataset shape is now rows: {} cols:{}'.format(scalar_coupling_contributions.shape[0],\\n                                                                                   scalar_coupling_contributions.shape[1]))\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "size = round(0.10*train.shape[0])\n",
    "train = train[:size]\n",
    "test = test[:size]\n",
    "sub = sub[:size]\n",
    "structures = structures[:size]\n",
    "scalar_coupling_contributions = scalar_coupling_contributions[:size]\n",
    "\n",
    "print('Train dataset shape is now rows: {} cols:{}'.format(train.shape[0],train.shape[1]))\n",
    "print('Test dataset shape is now rows: {} cols:{}'.format(test.shape[0],test.shape[1]))\n",
    "print('Sub dataset shape is now rows: {} cols:{}'.format(sub.shape[0],sub.shape[1]))\n",
    "print('Structures dataset shape is now rows: {} cols:{}'.format(structures.shape[0],structures.shape[1]))\n",
    "print('Scalar_coupling_contributions dataset shape is now rows: {} cols:{}'.format(scalar_coupling_contributions.shape[0],\n",
    "                                                                                   scalar_coupling_contributions.shape[1]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importante things to know is that the scalar coupling constants in train.csv are a sum of four terms. \n",
    "```\n",
    "* fc is the Fermi Contact contribution\n",
    "* sd is the Spin-dipolar contribution\n",
    "* pso is the Paramagnetic spin-orbit contribution\n",
    "* dso is the Diamagnetic spin-orbit contribution. \n",
    "```\n",
    "Let's merge this into train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train, scalar_coupling_contributions, how = 'left',\n",
    "                  left_on  = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'],\n",
    "                  right_on = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "      <th>fc</th>\n",
       "      <th>sd</th>\n",
       "      <th>pso</th>\n",
       "      <th>dso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807599999999994</td>\n",
       "      <td>83.022400000000005</td>\n",
       "      <td>0.254579</td>\n",
       "      <td>1.25862</td>\n",
       "      <td>0.272010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.257000000000000</td>\n",
       "      <td>-11.034700000000001</td>\n",
       "      <td>0.352978</td>\n",
       "      <td>2.85839</td>\n",
       "      <td>-3.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254799999999999</td>\n",
       "      <td>-11.032500000000001</td>\n",
       "      <td>0.352944</td>\n",
       "      <td>2.85852</td>\n",
       "      <td>-3.433870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300000000001</td>\n",
       "      <td>-11.031900000000000</td>\n",
       "      <td>0.352934</td>\n",
       "      <td>2.85855</td>\n",
       "      <td>-3.433930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807400000000001</td>\n",
       "      <td>83.022199999999998</td>\n",
       "      <td>0.254585</td>\n",
       "      <td>1.25861</td>\n",
       "      <td>0.272013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254099999999999</td>\n",
       "      <td>-11.031700000000001</td>\n",
       "      <td>0.352932</td>\n",
       "      <td>2.85856</td>\n",
       "      <td>-3.433950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254799999999999</td>\n",
       "      <td>-11.032400000000001</td>\n",
       "      <td>0.352943</td>\n",
       "      <td>2.85853</td>\n",
       "      <td>-3.433870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809299999999993</td>\n",
       "      <td>83.024100000000004</td>\n",
       "      <td>0.254634</td>\n",
       "      <td>1.25856</td>\n",
       "      <td>0.272012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300000000001</td>\n",
       "      <td>-11.031900000000000</td>\n",
       "      <td>0.352943</td>\n",
       "      <td>2.85856</td>\n",
       "      <td>-3.433930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809500000000000</td>\n",
       "      <td>83.024299999999997</td>\n",
       "      <td>0.254628</td>\n",
       "      <td>1.25856</td>\n",
       "      <td>0.272012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     molecule_name  atom_index_0    ...           sd      pso       dso\n",
       "0   0  dsgdb9nsd_000001             1    ...     0.254579  1.25862  0.272010\n",
       "1   1  dsgdb9nsd_000001             1    ...     0.352978  2.85839 -3.433600\n",
       "2   2  dsgdb9nsd_000001             1    ...     0.352944  2.85852 -3.433870\n",
       "3   3  dsgdb9nsd_000001             1    ...     0.352934  2.85855 -3.433930\n",
       "4   4  dsgdb9nsd_000001             2    ...     0.254585  1.25861  0.272013\n",
       "5   5  dsgdb9nsd_000001             2    ...     0.352932  2.85856 -3.433950\n",
       "6   6  dsgdb9nsd_000001             2    ...     0.352943  2.85853 -3.433870\n",
       "7   7  dsgdb9nsd_000001             3    ...     0.254634  1.25856  0.272012\n",
       "8   8  dsgdb9nsd_000001             3    ...     0.352943  2.85856 -3.433930\n",
       "9   9  dsgdb9nsd_000001             4    ...     0.254628  1.25856  0.272012\n",
       "\n",
       "[10 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4658147</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4658148</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658149</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658150</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658151</td>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4658152</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4658153</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4658154</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4658155</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4658156</td>\n",
       "      <td>dsgdb9nsd_000015</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     molecule_name  atom_index_0  atom_index_1  type\n",
       "0  4658147  dsgdb9nsd_000004             2             0  2JHC\n",
       "1  4658148  dsgdb9nsd_000004             2             1  1JHC\n",
       "2  4658149  dsgdb9nsd_000004             2             3  3JHH\n",
       "3  4658150  dsgdb9nsd_000004             3             0  1JHC\n",
       "4  4658151  dsgdb9nsd_000004             3             1  2JHC\n",
       "5  4658152  dsgdb9nsd_000015             3             0  1JHC\n",
       "6  4658153  dsgdb9nsd_000015             3             2  3JHC\n",
       "7  4658154  dsgdb9nsd_000015             3             4  2JHH\n",
       "8  4658155  dsgdb9nsd_000015             3             5  2JHH\n",
       "9  4658156  dsgdb9nsd_000015             4             0  1JHC"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>fc</th>\n",
       "      <th>sd</th>\n",
       "      <th>pso</th>\n",
       "      <th>dso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>83.022400000000005</td>\n",
       "      <td>0.254579</td>\n",
       "      <td>1.25862</td>\n",
       "      <td>0.272010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.034700000000001</td>\n",
       "      <td>0.352978</td>\n",
       "      <td>2.85839</td>\n",
       "      <td>-3.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.032500000000001</td>\n",
       "      <td>0.352944</td>\n",
       "      <td>2.85852</td>\n",
       "      <td>-3.433870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.031900000000000</td>\n",
       "      <td>0.352934</td>\n",
       "      <td>2.85855</td>\n",
       "      <td>-3.433930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>83.022199999999998</td>\n",
       "      <td>0.254585</td>\n",
       "      <td>1.25861</td>\n",
       "      <td>0.272013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule_name  atom_index_0    ...         pso       dso\n",
       "0  dsgdb9nsd_000001             1    ...     1.25862  0.272010\n",
       "1  dsgdb9nsd_000001             1    ...     2.85839 -3.433600\n",
       "2  dsgdb9nsd_000001             1    ...     2.85852 -3.433870\n",
       "3  dsgdb9nsd_000001             1    ...     2.85855 -3.433930\n",
       "4  dsgdb9nsd_000001             2    ...     1.25861  0.272013\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_coupling_contributions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train['scalar_coupling_constant'] and scalar_coupling_contributions['fc']` quite similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "      <th>fc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.807599999999994</td>\n",
       "      <td>83.022400000000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-11.257000000000000</td>\n",
       "      <td>-11.034700000000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-11.254799999999999</td>\n",
       "      <td>-11.032500000000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-11.254300000000001</td>\n",
       "      <td>-11.031900000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.807400000000001</td>\n",
       "      <td>83.022199999999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-11.254099999999999</td>\n",
       "      <td>-11.031700000000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-11.254799999999999</td>\n",
       "      <td>-11.032400000000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>84.809299999999993</td>\n",
       "      <td>83.024100000000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-11.254300000000001</td>\n",
       "      <td>-11.031900000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>84.809500000000000</td>\n",
       "      <td>83.024299999999997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scalar_coupling_constant                  fc\n",
       "0        84.807599999999994  83.022400000000005\n",
       "1       -11.257000000000000 -11.034700000000001\n",
       "2       -11.254799999999999 -11.032500000000001\n",
       "3       -11.254300000000001 -11.031900000000000\n",
       "4        84.807400000000001  83.022199999999998\n",
       "5       -11.254099999999999 -11.031700000000001\n",
       "6       -11.254799999999999 -11.032400000000001\n",
       "7        84.809299999999993  83.024100000000004\n",
       "8       -11.254300000000001 -11.031900000000000\n",
       "9        84.809500000000000  83.024299999999997"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(objs=[train['scalar_coupling_constant'],scalar_coupling_contributions['fc'] ],axis=1)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based in others ideais we can:<br>\n",
    "\n",
    "- train a model to predict `fc` feature;\n",
    "- add this feature to train and test and train the same model to compare performance;\n",
    "- train a better model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id4\"></a> <br> \n",
    "# **4. Data Pre-processing** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use this great kernel to get x,y,z position. https://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 0.43, 'C': 0.8200000000000001, 'N': 0.8, 'O': 0.78, 'F': 0.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a440521ba1124734bae18cee10ad5971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2358657), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2730795f464948c0bd56248a16e3a44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2358657), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>EN</th>\n",
       "      <th>rad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.0126981359</td>\n",
       "      <td>1.0858041580</td>\n",
       "      <td>0.0080009958</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.0021504160</td>\n",
       "      <td>-0.0060313176</td>\n",
       "      <td>0.0019761204</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0117308430</td>\n",
       "      <td>1.4637511620</td>\n",
       "      <td>0.0002765748</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.5408150690</td>\n",
       "      <td>1.4475266140</td>\n",
       "      <td>-0.8766437152</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.5238136345</td>\n",
       "      <td>1.4379326440</td>\n",
       "      <td>0.9063972942</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule_name  atom_index atom  ...              z    EN   rad\n",
       "0  dsgdb9nsd_000001           0    C  ...   0.0080009958  2.55  0.82\n",
       "1  dsgdb9nsd_000001           1    H  ...   0.0019761204  2.20  0.43\n",
       "2  dsgdb9nsd_000001           2    H  ...   0.0002765748  2.20  0.43\n",
       "3  dsgdb9nsd_000001           3    H  ...  -0.8766437152  2.20  0.43\n",
       "4  dsgdb9nsd_000001           4    H  ...   0.9063972942  2.20  0.43\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "atomic_radius = {'H':0.38, 'C':0.77, 'N':0.75, 'O':0.73, 'F':0.71} # Without fudge factor\n",
    "\n",
    "fudge_factor = 0.05\n",
    "atomic_radius = {k:v + fudge_factor for k,v in atomic_radius.items()}\n",
    "print(atomic_radius)\n",
    "\n",
    "electronegativity = {'H':2.2, 'C':2.55, 'N':3.04, 'O':3.44, 'F':3.98}\n",
    "\n",
    "#structures = pd.read_csv(structures, dtype={'atom_index':np.int8})\n",
    "\n",
    "atoms = structures['atom'].values\n",
    "atoms_en = [electronegativity[x] for x in tqdm(atoms)]\n",
    "atoms_rad = [atomic_radius[x] for x in tqdm(atoms)]\n",
    "\n",
    "structures['EN'] = atoms_en\n",
    "structures['rad'] = atoms_rad\n",
    "\n",
    "display(structures.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chemical Bond Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating bonds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc5e41b63074cae801b75b7d4e324b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counting and condensing bonds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa32d3d1c8f24f23be32ac4096ac0db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2358657), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b68985a6984c8b8716c85a9ca0cb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2358657), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>EN</th>\n",
       "      <th>rad</th>\n",
       "      <th>n_bonds</th>\n",
       "      <th>bond_lengths_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.0126981359</td>\n",
       "      <td>1.0858041580</td>\n",
       "      <td>0.0080009958</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>4</td>\n",
       "      <td>1.091949701309204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.0021504160</td>\n",
       "      <td>-0.0060313176</td>\n",
       "      <td>0.0019761204</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091953039169312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0117308430</td>\n",
       "      <td>1.4637511620</td>\n",
       "      <td>0.0002765748</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091951608657837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.5408150690</td>\n",
       "      <td>1.4475266140</td>\n",
       "      <td>-0.8766437152</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091946363449097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>4</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.5238136345</td>\n",
       "      <td>1.4379326440</td>\n",
       "      <td>0.9063972942</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.091947555541992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.0404260543</td>\n",
       "      <td>1.0241077530</td>\n",
       "      <td>0.0625637998</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1.017194986343384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.0172574639</td>\n",
       "      <td>0.0125452063</td>\n",
       "      <td>-0.0273771593</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.017189979553223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>0.9157893661</td>\n",
       "      <td>1.3587451950</td>\n",
       "      <td>-0.0287577581</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.017187237739563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.5202777357</td>\n",
       "      <td>1.3435321260</td>\n",
       "      <td>-0.7755426124</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.017207860946655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dsgdb9nsd_000003</td>\n",
       "      <td>0</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.0343604951</td>\n",
       "      <td>0.9775395708</td>\n",
       "      <td>0.0076015923</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2</td>\n",
       "      <td>0.962106823921204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dsgdb9nsd_000003</td>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "      <td>0.0647664923</td>\n",
       "      <td>0.0205721989</td>\n",
       "      <td>0.0015346341</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.962106823921204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dsgdb9nsd_000003</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>0.8717903737</td>\n",
       "      <td>1.3007924050</td>\n",
       "      <td>0.0006931336</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.962106823921204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>0.5995394918</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>1.130589008331299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.5995394918</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>1.130589008331299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>-1.6616385860</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.062099099159241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dsgdb9nsd_000004</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>1.6616385860</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.062099099159241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dsgdb9nsd_000005</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.0133239314</td>\n",
       "      <td>1.1324657150</td>\n",
       "      <td>0.0082758861</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2</td>\n",
       "      <td>1.109173059463501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dsgdb9nsd_000005</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0023107217</td>\n",
       "      <td>-0.0191585871</td>\n",
       "      <td>0.0019287305</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1.151747941970825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dsgdb9nsd_000005</td>\n",
       "      <td>2</td>\n",
       "      <td>H</td>\n",
       "      <td>-0.0278026991</td>\n",
       "      <td>2.1989492960</td>\n",
       "      <td>0.0141537903</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1.066598057746887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dsgdb9nsd_000007</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>-0.0187040036</td>\n",
       "      <td>1.5255820150</td>\n",
       "      <td>0.0104328082</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.82</td>\n",
       "      <td>4</td>\n",
       "      <td>1.203627109527588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       molecule_name  atom_index        ...         n_bonds  bond_lengths_mean\n",
       "0   dsgdb9nsd_000001           0        ...               4  1.091949701309204\n",
       "1   dsgdb9nsd_000001           1        ...               1  1.091953039169312\n",
       "2   dsgdb9nsd_000001           2        ...               1  1.091951608657837\n",
       "3   dsgdb9nsd_000001           3        ...               1  1.091946363449097\n",
       "4   dsgdb9nsd_000001           4        ...               1  1.091947555541992\n",
       "5   dsgdb9nsd_000002           0        ...               3  1.017194986343384\n",
       "6   dsgdb9nsd_000002           1        ...               1  1.017189979553223\n",
       "7   dsgdb9nsd_000002           2        ...               1  1.017187237739563\n",
       "8   dsgdb9nsd_000002           3        ...               1  1.017207860946655\n",
       "9   dsgdb9nsd_000003           0        ...               2  0.962106823921204\n",
       "10  dsgdb9nsd_000003           1        ...               1  0.962106823921204\n",
       "11  dsgdb9nsd_000003           2        ...               1  0.962106823921204\n",
       "12  dsgdb9nsd_000004           0        ...               2  1.130589008331299\n",
       "13  dsgdb9nsd_000004           1        ...               2  1.130589008331299\n",
       "14  dsgdb9nsd_000004           2        ...               1  1.062099099159241\n",
       "15  dsgdb9nsd_000004           3        ...               1  1.062099099159241\n",
       "16  dsgdb9nsd_000005           0        ...               2  1.109173059463501\n",
       "17  dsgdb9nsd_000005           1        ...               1  1.151747941970825\n",
       "18  dsgdb9nsd_000005           2        ...               1  1.066598057746887\n",
       "19  dsgdb9nsd_000007           0        ...               4  1.203627109527588\n",
       "\n",
       "[20 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_atom = structures['atom_index'].values\n",
    "p = structures[['x', 'y', 'z']].values\n",
    "p_compare = p\n",
    "m = structures['molecule_name'].values\n",
    "m_compare = m\n",
    "r = structures['rad'].values\n",
    "r_compare = r\n",
    "\n",
    "source_row = np.arange(len(structures))\n",
    "max_atoms = 28\n",
    "\n",
    "bonds = np.zeros((len(structures)+1, max_atoms+1), dtype=np.int8)\n",
    "bond_dists = np.zeros((len(structures)+1, max_atoms+1), dtype=np.float32)\n",
    "\n",
    "print('Calculating bonds')\n",
    "\n",
    "for i in tqdm(range(max_atoms-1)):\n",
    "    p_compare = np.roll(p_compare, -1, axis=0)\n",
    "    m_compare = np.roll(m_compare, -1, axis=0)\n",
    "    r_compare = np.roll(r_compare, -1, axis=0)\n",
    "    \n",
    "    mask = np.where(m == m_compare, 1, 0) #Are we still comparing atoms in the same molecule?\n",
    "    dists = np.linalg.norm(p - p_compare, axis=1) * mask\n",
    "    r_bond = r + r_compare\n",
    "    \n",
    "    bond = np.where(np.logical_and(dists > 0.0001, dists < r_bond), 1, 0)\n",
    "    \n",
    "    source_row = source_row\n",
    "    target_row = source_row + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n",
    "    target_row = np.where(np.logical_or(target_row > len(structures), mask==0), len(structures), target_row) #If invalid target, write to dummy row\n",
    "    \n",
    "    source_atom = i_atom\n",
    "    target_atom = i_atom + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n",
    "    target_atom = np.where(np.logical_or(target_atom > max_atoms, mask==0), max_atoms, target_atom) #If invalid target, write to dummy col\n",
    "    \n",
    "    bonds[(source_row, target_atom)] = bond\n",
    "    bonds[(target_row, source_atom)] = bond\n",
    "    bond_dists[(source_row, target_atom)] = dists\n",
    "    bond_dists[(target_row, source_atom)] = dists\n",
    "\n",
    "bonds = np.delete(bonds, axis=0, obj=-1) #Delete dummy row\n",
    "bonds = np.delete(bonds, axis=1, obj=-1) #Delete dummy col\n",
    "bond_dists = np.delete(bond_dists, axis=0, obj=-1) #Delete dummy row\n",
    "bond_dists = np.delete(bond_dists, axis=1, obj=-1) #Delete dummy col\n",
    "\n",
    "print('Counting and condensing bonds')\n",
    "\n",
    "bonds_numeric = [[i for i,x in enumerate(row) if x] for row in tqdm(bonds)]\n",
    "bond_lengths = [[dist for i,dist in enumerate(row) if i in bonds_numeric[j]] for j,row in enumerate(tqdm(bond_dists))]\n",
    "bond_lengths_mean = [ np.mean(x) for x in bond_lengths]\n",
    "n_bonds = [len(x) for x in bonds_numeric]\n",
    "\n",
    "#bond_data = {'bond_' + str(i):col for i, col in enumerate(np.transpose(bonds))}\n",
    "#bond_data.update({'bonds_numeric':bonds_numeric, 'n_bonds':n_bonds})\n",
    "\n",
    "bond_data = {'n_bonds':n_bonds, 'bond_lengths_mean': bond_lengths_mean }\n",
    "bond_df = pd.DataFrame(bond_data)\n",
    "structures = structures.join(bond_df)\n",
    "display(structures.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info(df, atom_idx):\n",
    "    df = pd.merge(df, structures, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    \n",
    "    #df = df.drop('atom_index', axis=1)\n",
    "    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                            'x': f'x_{atom_idx}',\n",
    "                            'y': f'y_{atom_idx}',\n",
    "                            'z': f'z_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "train = map_atom_info(train, 0)\n",
    "train = map_atom_info(train, 1)\n",
    "\n",
    "test = map_atom_info(test, 0)\n",
    "test = map_atom_info(test, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the distance between atoms first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "train_p_0 = train[['x_0', 'y_0', 'z_0']].values\n",
    "train_p_1 = train[['x_1', 'y_1', 'z_1']].values\n",
    "test_p_0 = test[['x_0', 'y_0', 'z_0']].values\n",
    "test_p_1 = test[['x_1', 'y_1', 'z_1']].values\n",
    "\n",
    "train['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\n",
    "test['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\n",
    "train['dist_x'] = (train['x_0'] - train['x_1']) ** 2\n",
    "test['dist_x'] = (test['x_0'] - test['x_1']) ** 2\n",
    "train['dist_y'] = (train['y_0'] - train['y_1']) ** 2\n",
    "test['dist_y'] = (test['y_0'] - test['y_1']) ** 2\n",
    "train['dist_z'] = (train['z_0'] - train['z_1']) ** 2\n",
    "test['dist_z'] = (test['z_0'] - test['z_1']) ** 2\n",
    "\n",
    "train['type_0'] = train['type'].apply(lambda x: x[0])\n",
    "test['type_0'] = test['type'].apply(lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 932.89 Mb (69.8% reduction)\n",
      "Mem. usage decreased to 468.34 Mb (70.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "def create_features(df):\n",
    "    df['molecule_couples'] = df.groupby('molecule_name')['id'].transform('count')\n",
    "    df['molecule_dist_mean'] = df.groupby('molecule_name')['dist'].transform('mean')\n",
    "    df['molecule_dist_min'] = df.groupby('molecule_name')['dist'].transform('min')\n",
    "    df['molecule_dist_max'] = df.groupby('molecule_name')['dist'].transform('max')\n",
    "    df['atom_0_couples_count'] = df.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\n",
    "    df['atom_1_couples_count'] = df.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n",
    "    df[f'molecule_atom_index_0_x_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['x_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_y_1_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_y_1_mean_diff'] = df[f'molecule_atom_index_0_y_1_mean'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_mean_div'] = df[f'molecule_atom_index_0_y_1_mean'] / df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_max'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('max')\n",
    "    df[f'molecule_atom_index_0_y_1_max_diff'] = df[f'molecule_atom_index_0_y_1_max'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_z_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['z_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_dist_mean_diff'] = df[f'molecule_atom_index_0_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_mean_div'] = df[f'molecule_atom_index_0_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_0_dist_max_diff'] = df[f'molecule_atom_index_0_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max_div'] = df[f'molecule_atom_index_0_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_0_dist_min_diff'] = df[f'molecule_atom_index_0_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min_div'] = df[f'molecule_atom_index_0_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_std_diff'] = df[f'molecule_atom_index_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std_div'] = df[f'molecule_atom_index_0_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_1_dist_mean_diff'] = df[f'molecule_atom_index_1_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean_div'] = df[f'molecule_atom_index_1_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_1_dist_max_diff'] = df[f'molecule_atom_index_1_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max_div'] = df[f'molecule_atom_index_1_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_1_dist_min_diff'] = df[f'molecule_atom_index_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min_div'] = df[f'molecule_atom_index_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_1_dist_std_diff'] = df[f'molecule_atom_index_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std_div'] = df[f'molecule_atom_index_1_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_mean'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_1_dist_min'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_1_dist_min_diff'] = df[f'molecule_atom_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_1_dist_min_div'] = df[f'molecule_atom_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_std'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_1_dist_std_diff'] = df[f'molecule_atom_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_0_dist_std'] = df.groupby(['molecule_name', 'type_0'])['dist'].transform('std')\n",
    "    df[f'molecule_type_0_dist_std_diff'] = df[f'molecule_type_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean'] = df.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n",
    "    df[f'molecule_type_dist_mean_diff'] = df[f'molecule_type_dist_mean'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean_div'] = df[f'molecule_type_dist_mean'] / df['dist']\n",
    "    df[f'molecule_type_dist_max'] = df.groupby(['molecule_name', 'type'])['dist'].transform('max')\n",
    "    df[f'molecule_type_dist_min'] = df.groupby(['molecule_name', 'type'])['dist'].transform('min')\n",
    "    df[f'molecule_type_dist_std'] = df.groupby(['molecule_name', 'type'])['dist'].transform('std')\n",
    "    df[f'molecule_type_dist_std_diff'] = df[f'molecule_type_dist_std'] - df['dist']\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "train = create_features(train)\n",
    "test = create_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info(df_1,df_2, atom_idx):\n",
    "    df = pd.merge(df_1, df_2, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_closest(df_train):\n",
    "    #I apologize for my poor coding skill. Please make the better one.\n",
    "    df_temp=df_train.loc[:,[\"molecule_name\",\"atom_index_0\",\"atom_index_1\",\"dist\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n",
    "    df_temp_=df_temp.copy()\n",
    "    df_temp_= df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
    "                                       'atom_index_1': 'atom_index_0',\n",
    "                                       'x_0': 'x_1',\n",
    "                                       'y_0': 'y_1',\n",
    "                                       'z_0': 'z_1',\n",
    "                                       'x_1': 'x_0',\n",
    "                                       'y_1': 'y_0',\n",
    "                                       'z_1': 'z_0'})\n",
    "    df_temp=pd.concat(objs=[df_temp,df_temp_],axis=0)\n",
    "\n",
    "    df_temp[\"min_distance\"]=df_temp.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df_temp= df_temp[df_temp[\"min_distance\"]==df_temp[\"dist\"]]\n",
    "\n",
    "    df_temp=df_temp.drop(['x_0','y_0','z_0','min_distance'], axis=1)\n",
    "    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                     'atom_index_1': 'atom_index_closest',\n",
    "                                     'distance': 'distance_closest',\n",
    "                                     'x_1': 'x_closest',\n",
    "                                     'y_1': 'y_closest',\n",
    "                                     'z_1': 'z_closest'})\n",
    "\n",
    "    for atom_idx in [0,1]:\n",
    "        df_train = map_atom_info(df_train,df_temp, atom_idx)\n",
    "        df_train = df_train.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
    "                                            'distance_closest': f'distance_closest_{atom_idx}',\n",
    "                                            'x_closest': f'x_closest_{atom_idx}',\n",
    "                                            'y_closest': f'y_closest_{atom_idx}',\n",
    "                                            'z_closest': f'z_closest_{atom_idx}'})\n",
    "    return df_train\n",
    "\n",
    "#dtrain = create_closest(train)\n",
    "#dtest = create_closest(test)\n",
    "#print('dtrain size',dtrain.shape)\n",
    "#print('dtest size',dtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine angles calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cos_features(df):\n",
    "    df[\"distance_0\"]=((df['x_0']-df['x_closest_0'])**2+(df['y_0']-df['y_closest_0'])**2+(df['z_0']-df['z_closest_0'])**2)**(1/2)\n",
    "    df[\"distance_1\"]=((df['x_1']-df['x_closest_1'])**2+(df['y_1']-df['y_closest_1'])**2+(df['z_1']-df['z_closest_1'])**2)**(1/2)\n",
    "    df[\"vec_0_x\"]=(df['x_0']-df['x_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_0_y\"]=(df['y_0']-df['y_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_0_z\"]=(df['z_0']-df['z_closest_0'])/df[\"distance_0\"]\n",
    "    df[\"vec_1_x\"]=(df['x_1']-df['x_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_1_y\"]=(df['y_1']-df['y_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_1_z\"]=(df['z_1']-df['z_closest_1'])/df[\"distance_1\"]\n",
    "    df[\"vec_x\"]=(df['x_1']-df['x_0'])/df[\"dist\"]\n",
    "    df[\"vec_y\"]=(df['y_1']-df['y_0'])/df[\"dist\"]\n",
    "    df[\"vec_z\"]=(df['z_1']-df['z_0'])/df[\"dist\"]\n",
    "    df[\"cos_0_1\"]=df[\"vec_0_x\"]*df[\"vec_1_x\"]+df[\"vec_0_y\"]*df[\"vec_1_y\"]+df[\"vec_0_z\"]*df[\"vec_1_z\"]\n",
    "    df[\"cos_0\"]=df[\"vec_0_x\"]*df[\"vec_x\"]+df[\"vec_0_y\"]*df[\"vec_y\"]+df[\"vec_0_z\"]*df[\"vec_z\"]\n",
    "    df[\"cos_1\"]=df[\"vec_1_x\"]*df[\"vec_x\"]+df[\"vec_1_y\"]*df[\"vec_y\"]+df[\"vec_1_z\"]*df[\"vec_z\"]\n",
    "    df=df.drop(['vec_0_x','vec_0_y','vec_0_z','vec_1_x','vec_1_y','vec_1_z','vec_x','vec_y','vec_z'], axis=1)\n",
    "    return df\n",
    "    \n",
    "#train = add_cos_features(train)\n",
    "#test = add_cos_features(test)\n",
    "\n",
    "#print('train size',train.shape)\n",
    "#print('test size',test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping molecule_name and encode atom_0, atom_1 and type_0.<br>\n",
    "**@TODO:** Try others encoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_cols_list = ['id','molecule_name','sd','pso','dso']\n",
    "def del_cols(df, cols):\n",
    "    del_cols_list_ = [l for l in del_cols_list if l in df]\n",
    "    df = df.drop(del_cols_list_,axis=1)\n",
    "    return df\n",
    "\n",
    "train = del_cols(train,del_cols_list)\n",
    "test = del_cols(test,del_cols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categoric_single(df):\n",
    "    lbl = LabelEncoder()\n",
    "    cat_cols=[]\n",
    "    try:\n",
    "        cat_cols = df.describe(include=['O']).columns.tolist()\n",
    "        for cat in cat_cols:\n",
    "            df[cat] = lbl.fit_transform(list(df[cat].values))\n",
    "    except Exception as e:\n",
    "        print('error: ', str(e) )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categoric(dtrain,dtest):\n",
    "    lbl = LabelEncoder()\n",
    "    objs_n = len(dtrain)\n",
    "    dfmerge = pd.concat(objs=[dtrain,dtest],axis=0)\n",
    "    cat_cols=[]\n",
    "    try:\n",
    "        cat_cols = dfmerge.describe(include=['O']).columns.tolist()\n",
    "        for cat in cat_cols:\n",
    "            dfmerge[cat] = lbl.fit_transform(list(dfmerge[cat].values))\n",
    "    except Exception as e:\n",
    "        print('error: ', str(e) )\n",
    "\n",
    "    dtrain = dfmerge[:objs_n]\n",
    "    dtest = dfmerge[objs_n:]\n",
    "    return dtrain,dtest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = encode_categoric_single(train)\n",
    "test = encode_categoric_single(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fc = train['fc']\n",
    "X = train.drop(['scalar_coupling_constant','fc'],axis=1)\n",
    "y = train['scalar_coupling_constant']\n",
    "\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size (4658147, 79)\n",
      "X_test size (2505542, 79)\n",
      "dtest size (2505542, 79)\n",
      "y_fc size (4658147,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('X size',X.shape)\n",
    "print('X_test size',X_test.shape)\n",
    "print('dtest size',test.shape)\n",
    "print('y_fc size',y_fc.shape)\n",
    "\n",
    "del train, test\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_columns = ['bond_lengths_mean_y',\n",
    " 'molecule_atom_index_0_dist_max',\n",
    " 'bond_lengths_mean_x',\n",
    " 'molecule_atom_index_0_dist_mean',\n",
    " 'molecule_atom_index_0_dist_std',\n",
    " 'molecule_couples',\n",
    " 'molecule_atom_index_0_y_1_std',\n",
    " 'molecule_dist_mean',\n",
    " 'molecule_dist_max',\n",
    " 'dist_y',\n",
    " 'molecule_atom_index_0_z_1_std',\n",
    " 'molecule_atom_index_1_dist_max',\n",
    " 'molecule_atom_index_1_dist_min',\n",
    " 'molecule_atom_index_0_x_1_std',\n",
    " 'molecule_atom_index_1_dist_std',\n",
    " 'molecule_atom_index_0_y_1_mean_div',\n",
    " 'y_0',\n",
    " 'molecule_atom_index_1_dist_mean',\n",
    " 'molecule_atom_1_dist_mean',\n",
    " 'x_0',\n",
    " 'dist_x',\n",
    " 'molecule_type_dist_std',\n",
    " 'dist_z',\n",
    " 'molecule_atom_index_1_dist_std_diff',\n",
    " 'molecule_type_dist_mean_diff',\n",
    " 'molecule_atom_index_0_dist_max_div',\n",
    " 'molecule_atom_1_dist_std',\n",
    " 'molecule_type_0_dist_std',\n",
    " 'z_0',\n",
    " 'molecule_type_dist_std_diff',\n",
    " 'molecule_atom_index_0_y_1_mean_diff',\n",
    " 'molecule_atom_index_0_dist_std_diff',\n",
    " 'molecule_atom_index_0_dist_mean_div',\n",
    " 'molecule_atom_index_0_dist_max_diff',\n",
    " 'x_1',\n",
    " 'molecule_type_dist_max',\n",
    " 'molecule_atom_index_0_dist_std_div',\n",
    " 'molecule_atom_index_0_dist_mean_diff',\n",
    " 'molecule_atom_1_dist_std_diff',\n",
    " 'molecule_atom_index_0_y_1_max_diff',\n",
    " 'z_1',\n",
    " 'molecule_atom_index_0_y_1_max',\n",
    " 'molecule_atom_index_0_y_1_mean',\n",
    " 'y_1',\n",
    " 'molecule_type_0_dist_std_diff',\n",
    " 'molecule_dist_min',\n",
    " 'molecule_atom_index_1_dist_std_div',\n",
    " 'molecule_atom_1_dist_min',\n",
    " 'molecule_atom_index_1_dist_max_diff','type']\n",
    "\n",
    "X = X[good_columns].copy()\n",
    "X_test = X_test[good_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id5\"></a> <br> \n",
    "# **5. Model** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 3\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create out of fold feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Thu Jun 20 01:45:26 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.23537\tvalid_1's l1: 1.25803\n",
      "[1000]\ttraining's l1: 1.10539\tvalid_1's l1: 1.14623\n",
      "[1500]\ttraining's l1: 1.03041\tvalid_1's l1: 1.08684\n",
      "[2000]\ttraining's l1: 0.975876\tvalid_1's l1: 1.04676\n",
      "[2500]\ttraining's l1: 0.933263\tvalid_1's l1: 1.01742\n",
      "[3000]\ttraining's l1: 0.897684\tvalid_1's l1: 0.994291\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.897684\tvalid_1's l1: 0.994291\n",
      "Fold 2 started at Thu Jun 20 02:05:23 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.23369\tvalid_1's l1: 1.25801\n",
      "[1000]\ttraining's l1: 1.10276\tvalid_1's l1: 1.14434\n",
      "[1500]\ttraining's l1: 1.02772\tvalid_1's l1: 1.08488\n",
      "[2000]\ttraining's l1: 0.974929\tvalid_1's l1: 1.04635\n",
      "[2500]\ttraining's l1: 0.931313\tvalid_1's l1: 1.01582\n",
      "[3000]\ttraining's l1: 0.89601\tvalid_1's l1: 0.993103\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.89601\tvalid_1's l1: 0.993103\n",
      "Fold 3 started at Thu Jun 20 02:25:18 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.2321\tvalid_1's l1: 1.25659\n",
      "[1000]\ttraining's l1: 1.10212\tvalid_1's l1: 1.14408\n",
      "[1500]\ttraining's l1: 1.02926\tvalid_1's l1: 1.0874\n",
      "[2000]\ttraining's l1: 0.974672\tvalid_1's l1: 1.04699\n",
      "[2500]\ttraining's l1: 0.931167\tvalid_1's l1: 1.01644\n",
      "[3000]\ttraining's l1: 0.895356\tvalid_1's l1: 0.993243\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.895356\tvalid_1's l1: 0.993243\n",
      "CV mean score: -0.1593, std: 0.0012.\n"
     ]
    }
   ],
   "source": [
    "params = {'num_leaves': 50,\n",
    "          'min_child_samples': 79,\n",
    "          'min_data_in_leaf' : 100,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.2,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 1.0\n",
    "         }\n",
    "result_dict_lgb_oof = train_model_regression(X=X, X_test=X_test, y=y_fc, params=params, folds=folds, model_type='lgb', eval_metric='group_mae', plot_feature_importance=False,\n",
    "                                                      verbose=500, early_stopping_rounds=200, n_estimators=n_estimators_default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['oof_fc'] = result_dict_lgb_oof['oof']\n",
    "X_test['oof_fc'] = result_dict_lgb_oof['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_columns = ['oof_fc',\n",
    " 'bond_lengths_mean_y',\n",
    " 'molecule_atom_index_0_dist_max',\n",
    " 'bond_lengths_mean_x',\n",
    " 'molecule_atom_index_0_dist_mean',\n",
    " 'molecule_atom_index_0_dist_std',\n",
    " 'molecule_couples',\n",
    " 'molecule_atom_index_0_y_1_std',\n",
    " 'molecule_dist_mean',\n",
    " 'molecule_dist_max',\n",
    " 'dist_y',\n",
    " 'molecule_atom_index_0_z_1_std',\n",
    " 'molecule_atom_index_1_dist_max',\n",
    " 'molecule_atom_index_1_dist_min',\n",
    " 'molecule_atom_index_0_x_1_std',\n",
    " 'molecule_atom_index_1_dist_std',\n",
    " 'molecule_atom_index_0_y_1_mean_div',\n",
    " 'y_0',\n",
    " 'molecule_atom_index_1_dist_mean',\n",
    " 'molecule_atom_1_dist_mean',\n",
    " 'x_0',\n",
    " 'dist_x',\n",
    " 'molecule_type_dist_std',\n",
    " 'dist_z',\n",
    " 'molecule_atom_index_1_dist_std_diff',\n",
    " 'molecule_type_dist_mean_diff',\n",
    " 'molecule_atom_index_0_dist_max_div',\n",
    " 'molecule_atom_1_dist_std',\n",
    " 'molecule_type_0_dist_std',\n",
    " 'z_0',\n",
    " 'molecule_type_dist_std_diff',\n",
    " 'molecule_atom_index_0_y_1_mean_diff',\n",
    " 'molecule_atom_index_0_dist_std_diff',\n",
    " 'molecule_atom_index_0_dist_mean_div',\n",
    " 'molecule_atom_index_0_dist_max_diff',\n",
    " 'x_1',\n",
    " 'molecule_type_dist_max',\n",
    " 'molecule_atom_index_0_dist_std_div',\n",
    " 'molecule_atom_index_0_dist_mean_diff',\n",
    " 'molecule_atom_1_dist_std_diff',\n",
    " 'molecule_atom_index_0_y_1_max_diff',\n",
    " 'z_1',\n",
    " 'molecule_atom_index_0_y_1_max',\n",
    " 'molecule_atom_index_0_y_1_mean',\n",
    " 'y_1',\n",
    " 'molecule_type_0_dist_std_diff',\n",
    " 'molecule_dist_min',\n",
    " 'molecule_atom_index_1_dist_std_div',\n",
    " 'molecule_atom_1_dist_min',\n",
    " 'molecule_atom_index_1_dist_max_diff','type']\n",
    "\n",
    "X = X[good_columns].copy()\n",
    "X_test = X_test[good_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bunch_of_features(dtrain,dtest,cat_features):\n",
    "    n_new_features = 0\n",
    "    train_objs_num = len(dtrain)\n",
    "    df_merge = pd.concat(objs=[dtrain, dtest], axis=0)\n",
    "        \n",
    "    for feature in cat_features:\n",
    "            #Log Transform\n",
    "            df_merge[feature+'_log'] = np.log (1 + df_merge[feature])\n",
    "            n_new_features = n_new_features +1\n",
    "\n",
    "    dtrain = df_merge[:train_objs_num]\n",
    "    dtest = df_merge[train_objs_num:]\n",
    "    del df_merge\n",
    "    gc.collect()\n",
    "    print('Features Created: {} \\nTotal Features {}'.format(n_new_features,len(dtrain.columns)))\n",
    "    return dtrain,dtest\n",
    "\n",
    "#features = list(X.columns)\n",
    "#X, X_test = create_bunch_of_features(X,X_test,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Best Feature for Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 128,\n",
    "          'min_child_samples': 79,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.2,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 1.0\n",
    "         }\n",
    "#result_dict_lgb2 = train_model_regression(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='group_mae', plot_feature_importance=True,\n",
    "#                                                      verbose=500, early_stopping_rounds=200, n_estimators=n_estimators_default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nfeature_importance = result_dict_lgb2['feature_importance']\\nbest_features = feature_importance[['feature','importance']].groupby(['feature']).mean().sort_values(\\n        by='importance',ascending=False).iloc[:50,0:0].index.tolist()\\nbest_features\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best Features? \n",
    "''' \n",
    "feature_importance = result_dict_lgb2['feature_importance']\n",
    "best_features = feature_importance[['feature','importance']].groupby(['feature']).mean().sort_values(\n",
    "        by='importance',ascending=False).iloc[:50,0:0].index.tolist()\n",
    "best_features'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id6\"></a> <br> \n",
    "# **6. Final Model** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models for each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of type 0\n",
      "Fold 1 started at Thu Jun 20 02:45:33 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.29748\tvalid_1's l1: 1.59058\n",
      "[1000]\ttraining's l1: 1.02789\tvalid_1's l1: 1.5289\n",
      "[1500]\ttraining's l1: 0.838422\tvalid_1's l1: 1.48906\n",
      "[2000]\ttraining's l1: 0.696969\tvalid_1's l1: 1.46429\n",
      "[2500]\ttraining's l1: 0.587396\tvalid_1's l1: 1.44756\n",
      "[3000]\ttraining's l1: 0.500955\tvalid_1's l1: 1.43526\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.500955\tvalid_1's l1: 1.43526\n",
      "Fold 2 started at Thu Jun 20 02:51:52 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.28646\tvalid_1's l1: 1.59573\n",
      "[1000]\ttraining's l1: 1.01913\tvalid_1's l1: 1.53281\n",
      "[1500]\ttraining's l1: 0.831583\tvalid_1's l1: 1.49459\n",
      "[2000]\ttraining's l1: 0.692051\tvalid_1's l1: 1.47032\n",
      "[2500]\ttraining's l1: 0.582964\tvalid_1's l1: 1.45358\n",
      "[3000]\ttraining's l1: 0.496579\tvalid_1's l1: 1.44245\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.496579\tvalid_1's l1: 1.44245\n",
      "Fold 3 started at Thu Jun 20 02:58:13 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 1.29479\tvalid_1's l1: 1.59866\n",
      "[1000]\ttraining's l1: 1.02568\tvalid_1's l1: 1.53532\n",
      "[1500]\ttraining's l1: 0.835688\tvalid_1's l1: 1.495\n",
      "[2000]\ttraining's l1: 0.693757\tvalid_1's l1: 1.46931\n",
      "[2500]\ttraining's l1: 0.585096\tvalid_1's l1: 1.45281\n",
      "[3000]\ttraining's l1: 0.498858\tvalid_1's l1: 1.44174\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.498858\tvalid_1's l1: 1.44174\n",
      "CV mean score: 0.3645, std: 0.0022.\n",
      "Training of type 3\n",
      "Fold 1 started at Thu Jun 20 03:04:23 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.248579\tvalid_1's l1: 0.370889\n",
      "[1000]\ttraining's l1: 0.171579\tvalid_1's l1: 0.351225\n",
      "[1500]\ttraining's l1: 0.126701\tvalid_1's l1: 0.344196\n",
      "[2000]\ttraining's l1: 0.0966678\tvalid_1's l1: 0.340717\n",
      "[2500]\ttraining's l1: 0.075151\tvalid_1's l1: 0.338827\n",
      "[3000]\ttraining's l1: 0.0587571\tvalid_1's l1: 0.337613\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.0587571\tvalid_1's l1: 0.337613\n",
      "Fold 2 started at Thu Jun 20 03:08:17 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.246318\tvalid_1's l1: 0.372075\n",
      "[1000]\ttraining's l1: 0.170408\tvalid_1's l1: 0.352281\n",
      "[1500]\ttraining's l1: 0.125565\tvalid_1's l1: 0.344637\n",
      "[2000]\ttraining's l1: 0.09552\tvalid_1's l1: 0.340631\n",
      "[2500]\ttraining's l1: 0.0738895\tvalid_1's l1: 0.338508\n",
      "[3000]\ttraining's l1: 0.0581285\tvalid_1's l1: 0.337256\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.0581285\tvalid_1's l1: 0.337256\n",
      "Fold 3 started at Thu Jun 20 03:12:11 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.248881\tvalid_1's l1: 0.374139\n",
      "[1000]\ttraining's l1: 0.171995\tvalid_1's l1: 0.354095\n",
      "[1500]\ttraining's l1: 0.127863\tvalid_1's l1: 0.346715\n",
      "[2000]\ttraining's l1: 0.0974023\tvalid_1's l1: 0.343148\n",
      "[2500]\ttraining's l1: 0.0755893\tvalid_1's l1: 0.341035\n",
      "[3000]\ttraining's l1: 0.0592109\tvalid_1's l1: 0.339608\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.0592109\tvalid_1's l1: 0.339608\n",
      "CV mean score: -1.0842, std: 0.0031.\n",
      "Training of type 1\n",
      "Fold 1 started at Thu Jun 20 03:16:10 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.241771\tvalid_1's l1: 0.646707\n",
      "[1000]\ttraining's l1: 0.102734\tvalid_1's l1: 0.631295\n",
      "[1500]\ttraining's l1: 0.0444509\tvalid_1's l1: 0.627393\n",
      "[2000]\ttraining's l1: 0.0198237\tvalid_1's l1: 0.626197\n",
      "[2500]\ttraining's l1: 0.00848043\tvalid_1's l1: 0.625763\n",
      "[3000]\ttraining's l1: 0.004035\tvalid_1's l1: 0.625594\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.004035\tvalid_1's l1: 0.625594\n",
      "Fold 2 started at Thu Jun 20 03:17:08 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.242108\tvalid_1's l1: 0.651558\n",
      "[1000]\ttraining's l1: 0.103092\tvalid_1's l1: 0.635512\n",
      "[1500]\ttraining's l1: 0.0451569\tvalid_1's l1: 0.632018\n",
      "[2000]\ttraining's l1: 0.0201199\tvalid_1's l1: 0.631067\n",
      "[2500]\ttraining's l1: 0.00879591\tvalid_1's l1: 0.630561\n",
      "[3000]\ttraining's l1: 0.0042111\tvalid_1's l1: 0.630444\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.0042111\tvalid_1's l1: 0.630444\n",
      "Fold 3 started at Thu Jun 20 03:18:03 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.253697\tvalid_1's l1: 0.654475\n",
      "[1000]\ttraining's l1: 0.105285\tvalid_1's l1: 0.636052\n",
      "[1500]\ttraining's l1: 0.0468043\tvalid_1's l1: 0.632225\n",
      "[2000]\ttraining's l1: 0.0208471\tvalid_1's l1: 0.631054\n",
      "[2500]\ttraining's l1: 0.00907399\tvalid_1's l1: 0.630626\n",
      "[3000]\ttraining's l1: 0.00432031\tvalid_1's l1: 0.630484\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.00432031\tvalid_1's l1: 0.630484\n",
      "CV mean score: -0.4639, std: 0.0037.\n",
      "Training of type 4\n",
      "Fold 1 started at Thu Jun 20 03:19:00 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.168725\tvalid_1's l1: 0.397807\n",
      "[1000]\ttraining's l1: 0.0837682\tvalid_1's l1: 0.387487\n",
      "[1500]\ttraining's l1: 0.0450821\tvalid_1's l1: 0.384592\n",
      "[2000]\ttraining's l1: 0.0250795\tvalid_1's l1: 0.383465\n",
      "[2500]\ttraining's l1: 0.0143791\tvalid_1's l1: 0.382916\n",
      "[3000]\ttraining's l1: 0.00854199\tvalid_1's l1: 0.382715\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.00854199\tvalid_1's l1: 0.382715\n",
      "Fold 2 started at Thu Jun 20 03:20:50 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.174663\tvalid_1's l1: 0.403034\n",
      "[1000]\ttraining's l1: 0.086048\tvalid_1's l1: 0.390925\n",
      "[1500]\ttraining's l1: 0.0454008\tvalid_1's l1: 0.387586\n",
      "[2000]\ttraining's l1: 0.0251721\tvalid_1's l1: 0.386366\n",
      "[2500]\ttraining's l1: 0.0142851\tvalid_1's l1: 0.385912\n",
      "[3000]\ttraining's l1: 0.00852016\tvalid_1's l1: 0.385732\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.00852016\tvalid_1's l1: 0.385732\n",
      "Fold 3 started at Thu Jun 20 03:22:42 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.167607\tvalid_1's l1: 0.395742\n",
      "[1000]\ttraining's l1: 0.0832846\tvalid_1's l1: 0.386114\n",
      "[1500]\ttraining's l1: 0.0447508\tvalid_1's l1: 0.383246\n",
      "[2000]\ttraining's l1: 0.0249328\tvalid_1's l1: 0.382155\n",
      "[2500]\ttraining's l1: 0.0143471\tvalid_1's l1: 0.381628\n",
      "[3000]\ttraining's l1: 0.00857127\tvalid_1's l1: 0.381386\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.00857127\tvalid_1's l1: 0.381386\n",
      "CV mean score: -0.9590, std: 0.0047.\n",
      "Training of type 2\n",
      "Fold 1 started at Thu Jun 20 03:24:34 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.658441\tvalid_1's l1: 0.76777\n",
      "[1000]\ttraining's l1: 0.534774\tvalid_1's l1: 0.716643\n",
      "[1500]\ttraining's l1: 0.452647\tvalid_1's l1: 0.689575\n",
      "[2000]\ttraining's l1: 0.391384\tvalid_1's l1: 0.673315\n",
      "[2500]\ttraining's l1: 0.34279\tvalid_1's l1: 0.662028\n",
      "[3000]\ttraining's l1: 0.303332\tvalid_1's l1: 0.654806\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.303332\tvalid_1's l1: 0.654806\n",
      "Fold 2 started at Thu Jun 20 03:33:52 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.657635\tvalid_1's l1: 0.769486\n",
      "[1000]\ttraining's l1: 0.535111\tvalid_1's l1: 0.718452\n",
      "[1500]\ttraining's l1: 0.452732\tvalid_1's l1: 0.691325\n",
      "[2000]\ttraining's l1: 0.390796\tvalid_1's l1: 0.673658\n",
      "[2500]\ttraining's l1: 0.34257\tvalid_1's l1: 0.662388\n",
      "[3000]\ttraining's l1: 0.303085\tvalid_1's l1: 0.654742\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.303085\tvalid_1's l1: 0.654742\n",
      "Fold 3 started at Thu Jun 20 03:43:15 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.662275\tvalid_1's l1: 0.7711\n",
      "[1000]\ttraining's l1: 0.537656\tvalid_1's l1: 0.717856\n",
      "[1500]\ttraining's l1: 0.454699\tvalid_1's l1: 0.689628\n",
      "[2000]\ttraining's l1: 0.392534\tvalid_1's l1: 0.672304\n",
      "[2500]\ttraining's l1: 0.343887\tvalid_1's l1: 0.661054\n",
      "[3000]\ttraining's l1: 0.303548\tvalid_1's l1: 0.65291\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.303548\tvalid_1's l1: 0.65291\n",
      "CV mean score: -0.4244, std: 0.0013.\n",
      "Training of type 6\n",
      "Fold 1 started at Thu Jun 20 03:52:34 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.316433\tvalid_1's l1: 0.416042\n",
      "[1000]\ttraining's l1: 0.234234\tvalid_1's l1: 0.388292\n",
      "[1500]\ttraining's l1: 0.18352\tvalid_1's l1: 0.376302\n",
      "[2000]\ttraining's l1: 0.148321\tvalid_1's l1: 0.370256\n",
      "[2500]\ttraining's l1: 0.121773\tvalid_1's l1: 0.366672\n",
      "[3000]\ttraining's l1: 0.101318\tvalid_1's l1: 0.36411\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.101318\tvalid_1's l1: 0.36411\n",
      "Fold 2 started at Thu Jun 20 03:58:06 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.316116\tvalid_1's l1: 0.417274\n",
      "[1000]\ttraining's l1: 0.233325\tvalid_1's l1: 0.389466\n",
      "[1500]\ttraining's l1: 0.182957\tvalid_1's l1: 0.378067\n",
      "[2000]\ttraining's l1: 0.147678\tvalid_1's l1: 0.371791\n",
      "[2500]\ttraining's l1: 0.121385\tvalid_1's l1: 0.367939\n",
      "[3000]\ttraining's l1: 0.10088\tvalid_1's l1: 0.365497\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.10088\tvalid_1's l1: 0.365497\n",
      "Fold 3 started at Thu Jun 20 04:03:40 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.317269\tvalid_1's l1: 0.416084\n",
      "[1000]\ttraining's l1: 0.234162\tvalid_1's l1: 0.38746\n",
      "[1500]\ttraining's l1: 0.184096\tvalid_1's l1: 0.376038\n",
      "[2000]\ttraining's l1: 0.148821\tvalid_1's l1: 0.369749\n",
      "[2500]\ttraining's l1: 0.122245\tvalid_1's l1: 0.365847\n",
      "[3000]\ttraining's l1: 0.101748\tvalid_1's l1: 0.363281\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.101748\tvalid_1's l1: 0.363281\n",
      "CV mean score: -1.0098, std: 0.0025.\n",
      "Training of type 5\n",
      "Fold 1 started at Thu Jun 20 04:09:20 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.709225\tvalid_1's l1: 0.796514\n",
      "[1000]\ttraining's l1: 0.602832\tvalid_1's l1: 0.751339\n",
      "[1500]\ttraining's l1: 0.525412\tvalid_1's l1: 0.723942\n",
      "[2000]\ttraining's l1: 0.46475\tvalid_1's l1: 0.704196\n",
      "[2500]\ttraining's l1: 0.415769\tvalid_1's l1: 0.690394\n",
      "[3000]\ttraining's l1: 0.374926\tvalid_1's l1: 0.679877\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.374926\tvalid_1's l1: 0.679877\n",
      "Fold 2 started at Thu Jun 20 04:20:27 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.70802\tvalid_1's l1: 0.795716\n",
      "[1000]\ttraining's l1: 0.60196\tvalid_1's l1: 0.750761\n",
      "[1500]\ttraining's l1: 0.525244\tvalid_1's l1: 0.722729\n",
      "[2000]\ttraining's l1: 0.465151\tvalid_1's l1: 0.703545\n",
      "[2500]\ttraining's l1: 0.415711\tvalid_1's l1: 0.689358\n",
      "[3000]\ttraining's l1: 0.374916\tvalid_1's l1: 0.679016\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.374916\tvalid_1's l1: 0.679016\n",
      "Fold 3 started at Thu Jun 20 04:31:48 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.706508\tvalid_1's l1: 0.791906\n",
      "[1000]\ttraining's l1: 0.601128\tvalid_1's l1: 0.748282\n",
      "[1500]\ttraining's l1: 0.524545\tvalid_1's l1: 0.721572\n",
      "[2000]\ttraining's l1: 0.46473\tvalid_1's l1: 0.702281\n",
      "[2500]\ttraining's l1: 0.415166\tvalid_1's l1: 0.687838\n",
      "[3000]\ttraining's l1: 0.374753\tvalid_1's l1: 0.677945\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.374753\tvalid_1's l1: 0.677945\n",
      "CV mean score: -0.3872, std: 0.0012.\n",
      "Training of type 7\n",
      "Fold 1 started at Thu Jun 20 04:43:13 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.135772\tvalid_1's l1: 0.283488\n",
      "[1000]\ttraining's l1: 0.073515\tvalid_1's l1: 0.273716\n",
      "[1500]\ttraining's l1: 0.0426127\tvalid_1's l1: 0.270277\n",
      "[2000]\ttraining's l1: 0.0257542\tvalid_1's l1: 0.268876\n",
      "[2500]\ttraining's l1: 0.0160195\tvalid_1's l1: 0.268179\n",
      "[3000]\ttraining's l1: 0.0103484\tvalid_1's l1: 0.267788\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.0103484\tvalid_1's l1: 0.267788\n",
      "Fold 2 started at Thu Jun 20 04:45:29 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.136714\tvalid_1's l1: 0.284597\n",
      "[1000]\ttraining's l1: 0.0742557\tvalid_1's l1: 0.274226\n",
      "[1500]\ttraining's l1: 0.0435549\tvalid_1's l1: 0.27091\n",
      "[2000]\ttraining's l1: 0.0262644\tvalid_1's l1: 0.269316\n",
      "[2500]\ttraining's l1: 0.0164911\tvalid_1's l1: 0.268607\n",
      "[3000]\ttraining's l1: 0.0106238\tvalid_1's l1: 0.268256\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.0106238\tvalid_1's l1: 0.268256\n",
      "Fold 3 started at Thu Jun 20 04:47:43 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's l1: 0.137495\tvalid_1's l1: 0.284328\n",
      "[1000]\ttraining's l1: 0.0735836\tvalid_1's l1: 0.274279\n",
      "[1500]\ttraining's l1: 0.0427938\tvalid_1's l1: 0.271087\n",
      "[2000]\ttraining's l1: 0.0257932\tvalid_1's l1: 0.269622\n",
      "[2500]\ttraining's l1: 0.0160693\tvalid_1's l1: 0.268888\n",
      "[3000]\ttraining's l1: 0.0103497\tvalid_1's l1: 0.268518\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's l1: 0.0103497\tvalid_1's l1: 0.268518\n",
      "CV mean score: -1.3161, std: 0.0011.\n"
     ]
    }
   ],
   "source": [
    "X_short = pd.DataFrame({'ind': list(X.index), 'type': X['type'].values, 'oof': [0] * len(X), 'target': y.values})\n",
    "X_short_test = pd.DataFrame({'ind': list(X_test.index), 'type': X_test['type'].values, 'prediction': [0] * len(X_test)})\n",
    "for t in X['type'].unique():\n",
    "    print(f'Training of type {t}')\n",
    "    X_t = X.loc[X['type'] == t]\n",
    "    X_test_t = X_test.loc[X_test['type'] == t]\n",
    "    y_t = X_short.loc[X_short['type'] == t, 'target']\n",
    "    result_dict_lgb3 = train_model_regression(X=X_t, X_test=X_test_t, y=y_t, params=params, folds=folds, model_type='lgb', eval_metric='group_mae', plot_feature_importance=False,\n",
    "                                                      verbose=500, early_stopping_rounds=200, n_estimators=n_estimators_default)\n",
    "    X_short.loc[X_short['type'] == t, 'oof'] = result_dict_lgb3['oof']\n",
    "    X_short_test.loc[X_short_test['type'] == t, 'prediction'] = result_dict_lgb3['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"id7\"></a> <br> \n",
    "# **7. Submittion** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4658147</td>\n",
       "      <td>5.717604793869082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4658148</td>\n",
       "      <td>187.934171274228305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658149</td>\n",
       "      <td>7.069723222623435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658150</td>\n",
       "      <td>187.962602144986391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658151</td>\n",
       "      <td>6.094733765756864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  scalar_coupling_constant\n",
       "0  4658147         5.717604793869082\n",
       "1  4658148       187.934171274228305\n",
       "2  4658149         7.069723222623435\n",
       "3  4658150       187.962602144986391\n",
       "4  4658151         6.094733765756864"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training models for type\n",
    "sub['scalar_coupling_constant'] = X_short_test['prediction']\n",
    "sub.to_csv('submission_type.csv', index=False)\n",
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref\"></a> <br> \n",
    "# **8. References** \n",
    "\n",
    "[1] OOF Model: https://www.kaggle.com/adarshchavakula/out-of-fold-oof-model-cross-validation<br>\n",
    "[2] Using Meta Features: https://www.kaggle.com/artgor/using-meta-features-to-improve-model<br>\n",
    "[3] Lot of Features: https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b <br>\n",
    "[4] Angle Feature: https://www.kaggle.com/kmat2019/effective-feature <br>\n",
    "[5] Recovering bonds from structure: https://www.kaggle.com/aekoch95/bonds-from-structure-data <br>\n",
    "\n",
    "<h3 style=\"color:red\">If this kernel helps you, up vote to keep me motivated üòÅ<br>Thanks!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
